#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„ vLLM æµ‹è¯•è„šæœ¬
ä¸ä¾èµ–æ•´ä¸ªé¡¹ç›®ï¼Œåªæµ‹è¯• vLLM è¿æ¥å’ŒåŸºæœ¬åŠŸèƒ½
"""

import sys
import os
import json

def test_vllm_connection():
    """æµ‹è¯• vLLM è¿æ¥"""
    print("=" * 60)
    print("vLLM è¿æ¥æµ‹è¯•ï¼ˆç‹¬ç«‹æ¨¡å¼ï¼‰")
    print("=" * 60)
    print()
    
    # æµ‹è¯• OpenAI å®¢æˆ·ç«¯ï¼ˆvLLM ä½¿ç”¨ OpenAI å…¼å®¹ APIï¼‰
    try:
        from openai import OpenAI
        print("âœ… OpenAI å®¢æˆ·ç«¯å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥ OpenAI å®¢æˆ·ç«¯: {e}")
        print("   è¯·è¿è¡Œ: pip install openai")
        return False
    
    # æµ‹è¯•é…ç½®
    base_url = "http://localhost:8000/v1"
    api_key = "EMPTY"
    model = "Qwen/Qwen2.5-1.5B-Instruct"
    
    print(f"ğŸ”Œ è¿æ¥åœ°å€: {base_url}")
    print(f"ğŸ“¦ æ¨¡å‹: {model}")
    print()
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    try:
        client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        print("âœ… OpenAI å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ›å»ºå®¢æˆ·ç«¯: {e}")
        return False
    
    # æµ‹è¯•åˆ—å‡ºæ¨¡å‹
    print("\nğŸ“‹ æµ‹è¯•: åˆ—å‡ºå¯ç”¨æ¨¡å‹...")
    try:
        models = client.models.list()
        print(f"âœ… æˆåŠŸè¿æ¥åˆ° vLLM æœåŠ¡")
        print(f"   å¯ç”¨æ¨¡å‹æ•°: {len(models.data)}")
        for m in models.data:
            print(f"   - {m.id}")
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ—å‡ºæ¨¡å‹: {e}")
        print("\nğŸ’¡ æç¤º:")
        print("   1. ç¡®è®¤ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ")
        print("   2. è¿è¡Œ: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-1.5B-Instruct --port 8000 --trust-remote-code")
        print("   3. æˆ–è¿è¡Œ: ./start_vllm.sh")
        return False
    
    # æµ‹è¯•ç®€å•æŸ¥è¯¢
    print("\nğŸ§ª æµ‹è¯•: ç®€å•æŸ¥è¯¢...")
    test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç» Rust ç¼–ç¨‹è¯­è¨€ã€‚"
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "user", "content": test_prompt}
            ],
            max_tokens=100,
            temperature=0.7
        )
        
        if response.choices and response.choices[0].message.content:
            result = response.choices[0].message.content
            print(f"âœ… æŸ¥è¯¢æˆåŠŸ!")
            print(f"ğŸ“ å“åº”: {result[:200]}...")
            return True
        else:
            print("âŒ å“åº”ä¸ºç©º")
            return False
            
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   1. vLLM æœåŠ¡æœªå¯åŠ¨")
        print("   2. æ¨¡å‹åç§°ä¸åŒ¹é…")
        print("   3. ç«¯å£é…ç½®é”™è¯¯")
        return False


def test_config_file():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("é…ç½®æ–‡ä»¶æµ‹è¯•")
    print("=" * 60)
    print()
    
    config_files = ["sactor.toml", "sactor.default.toml"]
    
    for config_file in config_files:
        if os.path.exists(config_file):
            print(f"ğŸ“„ æ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_file}")
            try:
                import tomli
                with open(config_file, "rb") as f:
                    config = tomli.load(f)
                
                if 'VLLM' in config:
                    print(f"âœ… {config_file} åŒ…å« [VLLM] é…ç½®")
                    vllm_config = config['VLLM']
                    print(f"   base_url: {vllm_config.get('base_url', 'N/A')}")
                    print(f"   model: {vllm_config.get('model', 'N/A')}")
                else:
                    print(f"âš ï¸  {config_file} ä¸åŒ…å« [VLLM] é…ç½®")
                    
            except ImportError:
                print("âš ï¸  éœ€è¦ tomli åº“æ¥è¯»å–é…ç½®æ–‡ä»¶")
            except Exception as e:
                print(f"âš ï¸  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            break
    else:
        print("âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶")


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®
    test_config_file()
    
    # æµ‹è¯•è¿æ¥
    success = test_vllm_connection()
    
    print("\n" + "=" * 60)
    if success:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   1. é…ç½® sactor.toml:")
        print("      [general]")
        print("      llm = \"VLLM\"")
        print("   2. è¿è¡Œç¿»è¯‘å‘½ä»¤")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥!")
        print("\nğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. vLLM æœåŠ¡æ˜¯å¦è¿è¡Œ")
        print("   2. ç«¯å£æ˜¯å¦æ­£ç¡®ï¼ˆé»˜è®¤8000ï¼‰")
        print("   3. æ¨¡å‹æ˜¯å¦å·²åŠ è½½")
        sys.exit(1)

