import json
import os
import subprocess
from typing import override

from sactor import utils
from sactor.llm import LLM
from sactor.sactor import CParser

from .test_generator import TestGenerator


class ExecutableTestGenerator(TestGenerator):
    def __init__(
        self,
        llm: LLM,
        c_parser: CParser,
        test_samples,
        executable,
        feed_as_arguments=True,
        input_document=None,
    ):
        super().__init__(
            llm=llm,
            test_samples=test_samples,
            c_parser=c_parser,
            input_document=input_document
        )
        self.executable = executable
        self.feed_as_arguments = feed_as_arguments

        # Check if test samples are valid
        for sample in self.init_test_samples:
            self._execute_test_sample(sample)

    def _execute_test_sample(self, test_sample):
        # TODO: support error tests
        if self.feed_as_arguments:
            feed_input_str = f'{self.executable} {test_sample}'
            cmd = feed_input_str.split()
            result = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
        else:
            cmd = self.executable
            result = subprocess.run(
                cmd,
                input=test_sample.encode(),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            )
        if result.returncode != 0:
            raise ValueError(
                f"Failed to run the executable with the input: {test_sample}"
            )
        return utils.normalize_string(result.stdout.decode() + result.stderr.decode())

    @override
    def generate_tests(self, count):
        # Generate test cases
        prompt = ''
        system_message = "You are an expert to write end-to-end tests for a C program."

        main_code = self.c_parser.extract_function_code("main")
        if main_code is None:
            raise ValueError("No main function found in the C program")

        prompt += f'''
The C program has the following main function:
```c
{main_code}
```
'''
        if self.input_document:
            prompt += f'''
The C program has the following inormation in its documentation:
{self.input_document}
'''
        if len(self.test_samples) > 0:
            prompt += f'''
The C program has the following test cases already written:
'''
            for i, sample in enumerate(self.test_samples):
                prompt += f'''
----INPUT {i}----
```
{sample}
```
----END INPUT {i}----
'''

        prompt += f'''
Please write {count} test cases for the C program (start from i=1). All test cases should be written with the following format:
----INPUT {{i}}----
```
Your input i here, **DO NOT** provide any comments or extra information in this block
```
----END INPUT {{i}}----
----INPUT {{i+1}}----
```
Your input i+1 here, **DO NOT** provide any comments or extra information in this block
```
----END INPUT {{i+1}}----

You don't need to provide the expected output. The expected output will be generated by the system.
'''

        result = self.llm.query(prompt, override_system_message=system_message)
        success_count = 0
        for i in range(1, count + 1):
            try:
                test_case = utils.parse_llm_result(result, f"input {i}")
                self.test_samples.append(test_case[f"input {i}"].strip())
                success_count += 1
            except ValueError as _:
                return self.generate_tests(count - success_count)  # Retry

        # collect test cases
        for i, sample in enumerate(self.test_samples):
            output = self._execute_test_sample(sample)
            self.test_samples_output.append(
                {
                    "input": sample,
                    "output": output,
                }
            )

    @override
    def create_test_task(self, task_path, test_sample_path):
        '''
        Create the test task from the generated test samples
        '''
        self._check_runner_exist()

        # Write test samples to the test
        self.export_test_samples(test_sample_path)

        # Write the test task
        tasks = []
        for i in range(len(self.test_samples)):
            command = f'sactor-test-runner --type bin {os.path.abspath(test_sample_path)} %t {i}'
            if self.feed_as_arguments:
                command += f' --feed-as-args'
            else:
                command += f' --feed-as-stdin'
            tasks.append(
                {
                    "command": command,
                    "test_id": i,
                }
            )

        os.makedirs(os.path.dirname(task_path), exist_ok=True)
        with open(task_path, 'w') as f:
            json.dump(tasks, f, indent=4)
