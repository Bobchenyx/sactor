#!/usr/bin/env python3
"""
å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹è§£å†³æ–¹æ¡ˆ
1. æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
2. å¦‚æœæ²¡æœ‰ï¼Œè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
3. ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡Œ SACToR ç¿»è¯‘
"""

import os
import sys
import json
import subprocess
import tempfile
import shutil
import time
from pathlib import Path
from typing import Dict, List, Optional

class CompleteTestCaseSolution:
    """å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹è§£å†³æ–¹æ¡ˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–è§£å†³æ–¹æ¡ˆ"""
        self.temp_dir = tempfile.mkdtemp(prefix='complete_solution_')
        print(f"ğŸ“ ä¸´æ—¶å·¥ä½œç›®å½•: {self.temp_dir}")
        
        # é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ç›®å½•
        self.test_base_dir = "/home/changdi/sactor-datasets/Project_CodeNet/generated_tests"
        
        # å¯¼å…¥æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨
        sys.path.append('/home/changdi/sactor')
        from generate_test_cases import TestCaseGenerator
        self.test_generator = TestCaseGenerator()
    
    def __del__(self):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def check_pre_generated_tests(self, c_file_path: str) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        c_filename = os.path.basename(c_file_path)
        test_filename = c_filename + ".json"
        
        # æ£€æŸ¥ argv ç›®å½•
        argv_test_path = os.path.join(self.test_base_dir, "argv", test_filename)
        if os.path.exists(argv_test_path):
            return argv_test_path
        
        # æ£€æŸ¥ scanf ç›®å½•
        scanf_test_path = os.path.join(self.test_base_dir, "scanf", test_filename)
        if os.path.exists(scanf_test_path):
            return scanf_test_path
        
        return None
    
    def load_or_generate_test_cases(self, c_file_path: str, output_dir: str) -> tuple[str, str, bool]:
        """åŠ è½½é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æˆ–ç”Ÿæˆæ–°çš„æµ‹è¯•ç”¨ä¾‹"""
        c_filename = os.path.basename(c_file_path)
        test_samples_path = os.path.join(output_dir, "test_samples.json")
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        pregen_test_path = self.check_pre_generated_tests(c_file_path)
        
        if pregen_test_path:
            # ä½¿ç”¨é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            print(f"ğŸ¯ ä½¿ç”¨é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {os.path.basename(pregen_test_path)}")
            shutil.copy2(pregen_test_path, test_samples_path)
            used_pregen = True
        else:
            # ç”Ÿæˆæ–°çš„æµ‹è¯•ç”¨ä¾‹
            print(f"ğŸ”§ ç”Ÿæˆæ–°çš„æµ‹è¯•ç”¨ä¾‹: {c_filename}")
            test_samples = self.test_generator.generate_test_cases_for_file(c_file_path)
            
            if test_samples:
                # ä¿å­˜ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
                with open(test_samples_path, 'w') as f:
                    json.dump(test_samples, f, indent=2)
                print(f"âœ… ç”Ÿæˆäº† {len(test_samples)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
                used_pregen = False
            else:
                # ä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹
                print(f"âš ï¸ ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹")
                default_samples = [
                    {"input": "10", "output": "10"},
                    {"input": "5", "output": "5"},
                    {"input": "0", "output": "0"}
                ]
                with open(test_samples_path, 'w') as f:
                    json.dump(default_samples, f, indent=2)
                used_pregen = False
        
        # 2. åˆ›å»ºæµ‹è¯•ä»»åŠ¡æ–‡ä»¶
        test_task = []
        with open(test_samples_path, 'r') as f:
            test_samples = json.load(f)
        
        for i in range(len(test_samples)):
            test_task.append({
                "command": f"sactor run-tests --type bin ./test_samples.json %t {i} --feed-as-args",
                "test_id": i
            })
        
        test_task_path = os.path.join(output_dir, "test_task.json")
        with open(test_task_path, 'w') as f:
            json.dump(test_task, f, indent=2)
        
        return test_task_path, test_samples_path, used_pregen
    
    def translate_with_sactor_docker(self, c_file_path: str, output_dir: str, test_task_path: str) -> Dict:
        """ä½¿ç”¨ SACToR Docker è¿›è¡Œç¿»è¯‘"""
        try:
            # å¤åˆ¶ C æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
            c_file_dest = os.path.join(output_dir, os.path.basename(c_file_path))
            shutil.copy2(c_file_path, c_file_dest)
            
            # è¿è¡Œ SACToR Docker ç¿»è¯‘
            sactor_config = "/home/changdi/sactor/sactor.toml"
            cmd = [
                "docker", "run", "--rm",
                "-v", f"{sactor_config}:/app/sactor.toml",
                "-v", f"{output_dir}:/tmp/translation",
                "sactor", "translate",
                f"/tmp/translation/{os.path.basename(c_file_path)}",
                f"/tmp/translation/test_task.json",
                "--result-dir", "/tmp/translation/result",
                "--type", "bin"
            ]
            
            # ä½¿ç”¨è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            
            if result.returncode != 0:
                return {
                    'success': False,
                    'error': f"SACToR Docker ç¿»è¯‘å¤±è´¥: {result.stderr[:500]}...",
                    'result_dir': None
                }
            
            return {
                'success': True,
                'error': None,
                'result_dir': os.path.join(output_dir, "result")
            }
            
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'error': "SACToR Docker ç¿»è¯‘è¶…æ—¶ (10åˆ†é’Ÿ)",
                'result_dir': None
            }
        except Exception as e:
            return {
                'success': False,
                'error': f"SACToR Docker ç¿»è¯‘å‡ºé”™: {e}",
                'result_dir': None
            }
    
    def verify_translation_result(self, result_dir: str) -> Dict:
        """éªŒè¯ç¿»è¯‘ç»“æœ"""
        try:
            verification_results = {
                'unidiomatic': {'success': False, 'details': {}},
                'idiomatic': {'success': False, 'details': {}},
                'overall': False,
                'test_count': 0
            }
            
            # æŸ¥æ‰¾ç¿»è¯‘ç»“æœ
            unidiomatic_dir = os.path.join(result_dir, "translated_code_unidiomatic")
            idiomatic_dir = os.path.join(result_dir, "translated_code_idiomatic")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¿»è¯‘ç»“æœ
            if os.path.exists(unidiomatic_dir):
                verification_results['unidiomatic'] = {'success': True, 'details': {'exists': True}}
            
            if os.path.exists(idiomatic_dir):
                verification_results['idiomatic'] = {'success': True, 'details': {'exists': True}}
                
                # è®¡ç®—æµ‹è¯•æ•°é‡
                test_samples_path = os.path.join(result_dir, "..", "test_samples.json")
                if os.path.exists(test_samples_path):
                    with open(test_samples_path, 'r') as f:
                        test_samples = json.load(f)
                    verification_results['test_count'] = len(test_samples)
            
            # ç»¼åˆç»“æœ
            verification_results['overall'] = (
                verification_results['unidiomatic']['success'] and 
                verification_results['idiomatic']['success']
            )
            
            return verification_results
            
        except Exception as e:
            return {
                'unidiomatic': {'success': False, 'error': str(e)},
                'idiomatic': {'success': False, 'error': str(e)},
                'overall': False,
                'test_count': 0
            }
    
    def translate_and_verify(self, c_file_path: str, output_dir: str) -> Dict:
        """ç¿»è¯‘å’ŒéªŒè¯å•ä¸ª C æ–‡ä»¶"""
        try:
            print(f"ğŸ¯ å®Œæ•´è§£å†³æ–¹æ¡ˆç¿»è¯‘: {os.path.basename(c_file_path)}")
            
            # 1. åŠ è½½æˆ–ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            test_task_path, test_samples_path, used_pregen = self.load_or_generate_test_cases(c_file_path, output_dir)
            
            # 2. ä½¿ç”¨ SACToR Docker è¿›è¡Œç¿»è¯‘
            translation_result = self.translate_with_sactor_docker(c_file_path, output_dir, test_task_path)
            
            if not translation_result['success']:
                return {
                    'success': False,
                    'error': translation_result['error'],
                    'verification': None,
                    'test_count': 0,
                    'used_pregen': used_pregen
                }
            
            # 3. éªŒè¯ç¿»è¯‘ç»“æœ
            verification_results = self.verify_translation_result(translation_result['result_dir'])
            
            return {
                'success': True,
                'error': None,
                'verification': verification_results,
                'test_count': verification_results.get('test_count', 0),
                'result_dir': translation_result['result_dir'],
                'used_pregen': used_pregen
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f"ç¿»è¯‘å¤±è´¥: {e}",
                'verification': None,
                'test_count': 0,
                'used_pregen': False
            }
    
    def batch_translate(self, dataset_dirs: List[str], output_base_dir: str, max_files: int = None) -> Dict:
        """æ‰¹é‡ç¿»è¯‘"""
        all_c_files = []
        
        # æ”¶é›†æ‰€æœ‰ C æ–‡ä»¶
        for dataset_dir in dataset_dirs:
            if os.path.exists(dataset_dir):
                c_files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith('.c')]
                all_c_files.extend(c_files)
        
        # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if max_files is not None and len(all_c_files) > max_files:
            all_c_files = all_c_files[:max_files]
            print(f"âš ï¸ é™åˆ¶å¤„ç†å‰ {max_files} ä¸ªæ–‡ä»¶")
        
        total_files = len(all_c_files)
        print(f"ğŸš€ å¼€å§‹å®Œæ•´è§£å†³æ–¹æ¡ˆæ‰¹é‡ç¿»è¯‘ {total_files} ä¸ª C æ–‡ä»¶")
        
        results = {
            'total': total_files,
            'success': 0,
            'failed': 0,
            'verified': 0,
            'pre_generated_used': 0,
            'generated_used': 0,
            'default_used': 0,
            'details': [],
            'start_time': time.time()
        }
        
        for i, c_file_path in enumerate(all_c_files):
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i+1}/{total_files}: {os.path.basename(c_file_path)}")
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºè¾“å‡ºç›®å½•
            relative_path = os.path.relpath(c_file_path, os.path.dirname(dataset_dirs[0]))
            file_output_dir = os.path.join(output_base_dir, relative_path.replace('.c', ''))
            os.makedirs(file_output_dir, exist_ok=True)
            
            # ç¿»è¯‘å’ŒéªŒè¯
            result = self.translate_and_verify(c_file_path, file_output_dir)
            
            if result['success']:
                results['success'] += 1
                if result['verification'] and result['verification']['overall']:
                    results['verified'] += 1
            
            # ç»Ÿè®¡æµ‹è¯•ç”¨ä¾‹æ¥æº
            if result.get('used_pregen', False):
                results['pre_generated_used'] += 1
            else:
                results['generated_used'] += 1
            
            results['details'].append({
                'file': os.path.basename(c_file_path),
                'directory': os.path.dirname(c_file_path),
                'success': result['success'],
                'verified': result['verification']['overall'] if result['verification'] else False,
                'test_count': result['test_count'],
                'used_pregen': result.get('used_pregen', False),
                'error': result['error']
            })
            
            results['failed'] = results['total'] - results['success']
            
            # æ¯å¤„ç† 10 ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                self._save_progress(results, output_base_dir, i + 1)
        
        results['end_time'] = time.time()
        results['duration'] = results['end_time'] - results['start_time']
        
        return results
    
    def _save_progress(self, results: Dict, output_base_dir: str, processed_count: int):
        """ä¿å­˜è¿›åº¦"""
        progress_file = os.path.join(output_base_dir, f"progress_{processed_count}.json")
        with open(progress_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {processed_count}/{results['total']} æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    dataset_dirs = [
        "/home/changdi/sactor-datasets/Project_CodeNet/selected_data_raw/argv",
        "/home/changdi/sactor-datasets/Project_CodeNet/selected_data_raw/scanf"
    ]
    output_base_dir = "/home/changdi/sactor-datasets/sactor_complete_translations"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_base_dir, exist_ok=True)
    
    # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°å’Œé¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹è¦†ç›–
    total_files = 0
    pregen_argv = 0
    pregen_scanf = 0
    
    for dataset_dir in dataset_dirs:
        if os.path.exists(dataset_dir):
            c_files = [f for f in os.listdir(dataset_dir) if f.endswith('.c')]
            total_files += len(c_files)
            
            # ç»Ÿè®¡é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹è¦†ç›–
            if "argv" in dataset_dir:
                pregen_argv = len(os.listdir("/home/changdi/sactor-datasets/Project_CodeNet/generated_tests/argv"))
            elif "scanf" in dataset_dir:
                pregen_scanf = len(os.listdir("/home/changdi/sactor-datasets/Project_CodeNet/generated_tests/scanf"))
    
    print(f"ğŸ“ æ‰¾åˆ°æ€»è®¡ {total_files} ä¸ª C æ–‡ä»¶")
    print(f"   - argv ç›®å½•: {len([f for f in os.listdir(dataset_dirs[0]) if f.endswith('.c')])} ä¸ªæ–‡ä»¶")
    print(f"   - scanf ç›®å½•: {len([f for f in os.listdir(dataset_dirs[1]) if f.endswith('.c')])} ä¸ªæ–‡ä»¶")
    print(f"ğŸ¯ é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹è¦†ç›–:")
    print(f"   - argv: {pregen_argv} ä¸ªæµ‹è¯•æ–‡ä»¶")
    print(f"   - scanf: {pregen_scanf} ä¸ªæµ‹è¯•æ–‡ä»¶")
    print(f"ğŸ”§ å®Œæ•´è§£å†³æ–¹æ¡ˆ:")
    print(f"   - ä¼˜å…ˆä½¿ç”¨é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹")
    print(f"   - è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±çš„æµ‹è¯•ç”¨ä¾‹")
    print(f"   - ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½æœ‰æµ‹è¯•ç”¨ä¾‹")
    
    # åˆ›å»ºå®Œæ•´è§£å†³æ–¹æ¡ˆ
    solution = CompleteTestCaseSolution()
    
    try:
        # æ‰¹é‡ç¿»è¯‘
        results = solution.batch_translate(dataset_dirs, output_base_dir, max_files=None)
        
        # è¾“å‡ºç»“æœç»Ÿè®¡
        print(f"\nğŸ“Š SACToR å®Œæ•´è§£å†³æ–¹æ¡ˆæ‰¹é‡ç¿»è¯‘ç»“æœ:")
        print(f"æ€»æ–‡ä»¶æ•°: {results['total']}")
        print(f"ç¿»è¯‘æˆåŠŸ: {results['success']}")
        print(f"ç¿»è¯‘å¤±è´¥: {results['failed']}")
        print(f"éªŒè¯é€šè¿‡: {results['verified']}")
        print(f"ä½¿ç”¨é¢„ç”Ÿæˆæµ‹è¯•: {results['pre_generated_used']}")
        print(f"ä½¿ç”¨ç”Ÿæˆæµ‹è¯•: {results['generated_used']}")
        print(f"å¤„ç†æ—¶é—´: {results['duration']:.2f} ç§’")
        print(f"æˆåŠŸç‡: {results['success']/results['total']*100:.1f}%")
        print(f"éªŒè¯ç‡: {results['verified']/results['total']*100:.1f}%")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(output_base_dir, "sactor_complete_results.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return results
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if hasattr(solution, 'temp_dir') and os.path.exists(solution.temp_dir):
            shutil.rmtree(solution.temp_dir)

if __name__ == "__main__":
    main()
