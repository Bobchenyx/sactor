# 自动模型切换功能说明

## 功能特性

✨ **智能模型切换**：当检测到403错误或quota用完时，自动切换到下一个可用模型

✨ **断点续传**：中断后可以继续运行，已处理的文件不会重复处理

✨ **实时监控**：实时显示输出并监控错误

✨ **防止浪费**：用过的模型不会再次使用

## 运行方法

### 基本运行

```bash
cd /home/changdi/sactor
python3 batch_generate_with_auto_model_switch.py --workers 15 --num-tests 8
```

### 参数说明

- `--workers 15`: 并行线程数
- `--num-tests 8`: 每个文件生成的测试数量

## 工作流程

```
1. 启动 → 使用当前模型（qwen3-coder-plus）
   ↓
2. 运行批量生成
   ↓
3. 检测输出中的错误
   ↓
4. 如果检测到 1次 403/quota错误 ⚡
   ↓
5. 自动切换到下一个模型
   ↓
6. 等待5秒后重新启动
   ↓
7. 重复步骤2-6，直到完成或所有模型用完
```

## 可用模型列表

从 `sactor.toml` L68 读取：

1. qwen3-max
2. qwen3-max-preview
3. qwen3-coder-plus ⭐ (默认)
4. qwen-mt-plus
5. qwen-plus-latest
6. qwen3-max-2025-09-23
7. qwen3-coder-plus-2025-09-23
8. qwen-plus-2025-09-11
9. qwen-plus-2025-07-28
10. qwen3-coder-plus-2025-07-22
11. qwen-plus-2025-07-14
12. qwen-plus-2025-04-28
13. qwen3-coder-flash
14. qwen-plus-2025-01-25
15. qwen3-coder-flash-2025-07-28

**总共15个模型可轮换使用！**

## 自动切换示例

```
🚀 启动批量测试生成
   当前模型: qwen3-coder-plus
   
... 处理中 ...

⚠️  检测到错误: 403 Forbidden
⚠️  检测到错误: quota exceeded
⚠️  检测到错误: rate limit

================================================================================
⚠️  检测到多次配额错误，尝试切换模型...
================================================================================
❌ 模型 qwen3-coder-plus 配额已用完，标记为已使用
✅ 已切换到新模型: qwen3-max
   剩余可用模型: 13

等待5秒后重新开始...

🚀 启动批量测试生成
   当前模型: qwen3-max
   
... 继续处理 ...
```

## 模型切换逻辑

### 触发条件

- **⚡ 检测到1次**以下任一错误就立即切换：
  - `403` 状态码
  - `PermissionDeniedError` - OpenAI权限拒绝错误
  - `permission denied` - 权限拒绝
  - `quota` 关键词
  - `rate limit` 关键词

### 切换策略

1. 标记当前模型为"已使用"
2. 从可用模型列表中选择下一个未使用的
3. 更新 `sactor.toml` 中的 `model` 配置
4. 等待5秒（避免频繁切换）
5. 重新启动批量生成

### 已使用模型追踪

- ✅ 运行期间记录已使用的模型
- ✅ 不会重复使用同一个模型
- ✅ 如果所有模型都用完，会停止运行

## 优势

### 1. 最大化利用配额

通过轮换15个模型，可以处理大量文件：

| 场景 | 单模型 | 15模型轮换 |
|------|--------|-----------|
| 每模型配额 | ~5,000文件 | ~5,000文件 |
| **总处理能力** | **5,000** | **75,000+** |
| 成本 | 正常 | 正常 |

### 2. 无需人工干预

- ✅ 自动检测错误
- ✅ 自动切换模型
- ✅ 自动继续处理
- ✅ 无需babysit

### 3. 断点续传

- ✅ 已处理的文件会被跳过
- ✅ 可以随时Ctrl+C中断
- ✅ 重新运行会继续处理剩余文件

## 监控进度

### 实时查看日志

```bash
tail -f /home/changdi/sactor/generated_tests/logs/generation_log_*.csv
```

### 查看当前使用的模型

```bash
grep "^model =" /home/changdi/sactor/sactor.toml | grep -A 1 "Qwen"
```

### 统计已处理文件数

```bash
find /home/changdi/sactor/generated_tests -name "*.c.json" | wc -l
```

## 注意事项

### 1. 模型切换会中断当前处理

- 当前正在处理的15个文件会被中断
- 重新启动后会重新处理这些文件
- 通过断点续传机制，不会造成数据丢失

### 2. 快速响应机制 ⚡

- **检测到1次403错误就立即切换**
- 最小化失败的API调用
- 快速切换到可用模型，提高效率

### 3. 所有模型配额用完的情况

如果15个模型都用完了：

```bash
❌ 所有模型配额都已用完！
❌ 无法切换模型，停止运行

⚠️  任务未完全完成
```

解决方案：
- 等待配额重置（通常每天/每月）
- 联系Alibaba Cloud增加配额
- 使用其他API key

## 手动切换模型

如果需要手动切换模型：

```bash
# 编辑 sactor.toml
vim /home/changdi/sactor/sactor.toml

# 修改第64行
model = "qwen3-max"  # 改为你想用的模型

# 重新运行
python3 batch_generate_with_auto_model_switch.py --workers 15 --num-tests 8
```

## 对比原始脚本

| 特性 | 原始脚本 | 自动切换版 |
|------|---------|-----------|
| 配额用完处理 | ❌ 停止运行 | ✅ 自动切换 |
| 可处理文件数 | ~5,000 | ~75,000+ |
| 需要人工干预 | ✅ 是 | ❌ 否 |
| 断点续传 | ✅ 支持 | ✅ 支持 |
| 实时监控 | ✅ 支持 | ✅ 支持 |

## 建议使用场景

✅ **推荐使用自动切换版**：
- 处理大量文件（>5,000）
- 长时间运行（>10小时）
- 无人值守场景
- 配额可能不足

✅ **可以使用原始版**：
- 小批量测试（<1,000）
- 短时间运行
- 配额充足

## 总结

这个自动切换版本让你可以：

- 🔄 **自动轮换15个模型**
- 📈 **处理能力提升15倍**
- 🤖 **完全自动化，无需人工干预**
- 💾 **断点续传，随时可中断**
- 📊 **实时日志，进度可追踪**

**推荐用于大规模批量处理！** 🚀

