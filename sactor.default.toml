[general]
llm = "AzureOpenAI" # you only need to configure the llm you want to use
max_translation_attempts = 6
max_verifier_harness_attempts = 6
timeout_seconds = 60 # timeout for the execution of generated code
system_message = '''
You are an expert in translating code from C to Rust. You will take all information from the user as reference, and will output the translated code into the format that the user wants.
'''
encoding = "o200k_base" # Encoding for the `tiktoken` library, default for GPT-4o model

[AzureOpenAI]
api_key = "your-api-key"
endpoint = "https://your-endpoint.openai.azure.com/"
api_version = "2024-12-26" # change this to your own API version
model = "gpt-4o" # change this to your own model
# temperature = 1 # Uncomment this line if you want to set a temperature

[OpenAI]
api_key = "your-api-key"
model = "gpt-4o" # change this to your own model
# temperature = 1 # Uncomment this line if you want to set a temperature
# organization = "your-organization" # uncomment this line if you are using an organization
# project_id = "your-project-id" # uncomment this line if you are using a project id

[Anthropic]
api_key = "your-api-key"
model = "claude-3-5-sonnet-latest" # change this to your own model
max_tokens = 2048 # maximum number of tokens to generate for each request
# temperature  = 1 # Uncomment this line if you want to set a temperature

[Google]
api_key = "your-api-key"
model = 'gemini-2.0-flash-exp' # change this to your own model
# temperature = 1 # Uncomment this line if you want to set a temperature

[Ollama]
host = "http://127.0.0.1:11434"
model = "llama3.3"
# temperature = 0.6 # Uncomment this line if you want to set a temperature

[Ollama.headers]
# Add any headers you want to send with the request
# Authorization = "Bearer your-token"

[test_generator]
max_attempts = 6
timeout_seconds = 60

[test_runner]
timeout_seconds = 60
