#!/usr/bin/env python3
"""
ä½¿ç”¨æ­£ç¡®æ•°æ®ç›®å½•çš„ SACToR æ‰¹é‡ç¿»è¯‘è„šæœ¬
- åŸå§‹ç¨‹åº: /home/changdi/sactor-datasets/Project_CodeNet/raw_data/
- æµ‹è¯•ç”¨ä¾‹: /home/changdi/sactor-datasets/Project_CodeNet/generated_tests/
"""

import os
import sys
import json
import subprocess
import tempfile
import shutil
import time
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

class CorrectDataTranslator:
    """ä½¿ç”¨æ­£ç¡®æ•°æ®ç›®å½•çš„ SACToR æ‰¹é‡ç¿»è¯‘å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¿»è¯‘å™¨"""
        # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•
        self.temp_dir = tempfile.mkdtemp(prefix='sactor_correct_')
        print(f"ğŸ“ ä¸´æ—¶å·¥ä½œç›®å½•: {self.temp_dir}")
        
        # æ­£ç¡®çš„æ•°æ®ç›®å½•
        self.raw_data_dir = "/home/changdi/sactor-datasets/Project_CodeNet/raw_data"
        self.test_data_dir = "/home/changdi/sactor-datasets/Project_CodeNet/generated_tests"
    
    def __del__(self):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        if hasattr(self, 'temp_dir') and os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def _get_model_info(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯"""
        try:
            # è¯»å–sactor.tomlé…ç½®æ–‡ä»¶
            config_path = "/home/changdi/sactor/sactor.toml"
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    content = f.read()
                    # æŸ¥æ‰¾OpenAIæ¨¡å‹é…ç½®
                    if 'llm = "OpenAI"' in content:
                        # æŸ¥æ‰¾modelé…ç½® - æ›´ç²¾ç¡®çš„åŒ¹é…
                        import re
                        # æŸ¥æ‰¾[OpenAI]éƒ¨åˆ†ä¸‹çš„modelé…ç½®
                        openai_section = re.search(r'\[OpenAI\](.*?)(?=\[|$)', content, re.DOTALL)
                        if openai_section:
                            model_match = re.search(r'model = "([^"]+)"', openai_section.group(1))
                            if model_match:
                                return model_match.group(1)
                    # æŸ¥æ‰¾å…¶ä»–LLMé…ç½®
                    elif 'llm = "AzureOpenAI"' in content:
                        model_match = re.search(r'model = "([^"]+)"', content)
                        if model_match:
                            return f"AzureOpenAI-{model_match.group(1)}"
                    elif 'llm = "DeepSeek"' in content:
                        model_match = re.search(r'model = "([^"]+)"', content)
                        if model_match:
                            return f"DeepSeek-{model_match.group(1)}"
                    elif 'llm = "Anthropic"' in content:
                        model_match = re.search(r'model = "([^"]+)"', content)
                        if model_match:
                            return f"Anthropic-{model_match.group(1)}"
                    elif 'llm = "Google"' in content:
                        model_match = re.search(r'model = "([^"]+)"', content)
                        if model_match:
                            return f"Google-{model_match.group(1)}"
                    elif 'llm = "Ollama"' in content:
                        model_match = re.search(r'model = "([^"]+)"', content)
                        if model_match:
                            return f"Ollama-{model_match.group(1)}"
            return "Unknown"
        except Exception as e:
            print(f"âš ï¸ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return "Unknown"
    
    def log_translation_result(self, c_file_path: str, result: Dict, output_base_dir: str):
        """è®°å½•æ¯ä¸ªCæ–‡ä»¶çš„ç¿»è¯‘ç»“æœåˆ°æ—¥å¿—æ–‡ä»¶"""
        try:
            # åˆ›å»ºæ—¥å¿—ç›®å½•
            log_dir = os.path.join(output_base_dir, "logs")
            os.makedirs(log_dir, exist_ok=True)
            
            # åˆ›å»ºæ—¥å¿—æ–‡ä»¶åï¼ˆåŸºäºæ—¥æœŸå’Œæ¨¡å‹ï¼‰
            today = datetime.now().strftime("%Y-%m-%d")
            model_name = self._get_model_info().replace('-', '_').replace('.', '_')
            log_file = os.path.join(log_dir, f"translation_log_{today}_{model_name}.json")
            
            # å‡†å¤‡æ—¥å¿—æ¡ç›®
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "c_file": os.path.basename(c_file_path),
                "c_file_path": c_file_path,
                "success": result.get('success', False),
                "processing_time": result.get('processing_time', 0),
                "attempts": result.get('attempts', 0),
                "api_cost": result.get('api_cost', {}),
                "test_count": result.get('test_count', 0),
                "verified": result.get('verification', {}).get('overall', False) if result.get('verification') else False,
                "error": result.get('error', None),
                "model": self._get_model_info()
            }
            
            # è¯»å–ç°æœ‰æ—¥å¿—æˆ–åˆ›å»ºæ–°æ—¥å¿—
            if os.path.exists(log_file):
                with open(log_file, 'r') as f:
                    log_data = json.load(f)
            else:
                log_data = {
                    "session_info": {
                        "start_time": datetime.now().isoformat(),
                        "output_base_dir": output_base_dir
                    },
                    "translations": []
                }
            
            # æ·»åŠ æ–°æ¡ç›®
            log_data["translations"].append(log_entry)
            log_data["session_info"]["last_update"] = datetime.now().isoformat()
            
            # ä¿å­˜æ—¥å¿—
            with open(log_file, 'w') as f:
                json.dump(log_data, f, indent=2)
            
            # åŒæ—¶åˆ›å»ºCSVæ ¼å¼çš„æ—¥å¿—ï¼ˆä¾¿äºåˆ†æï¼‰
            csv_log_file = os.path.join(log_dir, f"translation_log_{today}_{model_name}.csv")
            
            # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ ‡é¢˜è¡Œ
            if not os.path.exists(csv_log_file):
                with open(csv_log_file, 'w') as f:
                    f.write("timestamp,c_file,success,processing_time,attempts,api_cost_total,test_count,verified,error,model\n")
            
            # è¿½åŠ CSVæ¡ç›®
            with open(csv_log_file, 'a') as f:
                api_cost_total = log_entry["api_cost"].get("total_cost", 0) if log_entry["api_cost"] else 0
                error_str = str(log_entry["error"]).replace(',', ';').replace('\n', ' ') if log_entry["error"] else ""
                model_str = str(log_entry["model"]).replace(',', ';').replace('\n', ' ') if log_entry["model"] else "Unknown"
                f.write(f"{log_entry['timestamp']},{log_entry['c_file']},{log_entry['success']},{log_entry['processing_time']:.2f},{log_entry['attempts']},{api_cost_total:.4f},{log_entry['test_count']},{log_entry['verified']},{error_str},{model_str}\n")
            
            print(f"ğŸ“ æ—¥å¿—å·²ä¿å­˜: {log_file}")
            
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—ä¿å­˜å¤±è´¥: {e}")
    
    def find_corresponding_test(self, c_file_path: str) -> Optional[str]:
        """æŸ¥æ‰¾å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶"""
        c_filename = os.path.basename(c_file_path)
        test_filename = c_filename + ".json"
        
        # ç¡®å®šå­ç›®å½• (argv æˆ– scanf)
        if "argv" in c_file_path:
            test_path = os.path.join(self.test_data_dir, "argv", test_filename)
        elif "scanf" in c_file_path:
            test_path = os.path.join(self.test_data_dir, "scanf", test_filename)
        else:
            return None
        
        if os.path.exists(test_path):
            return test_path
        return None
    
    def load_test_cases(self, test_file_path: str) -> List[Dict]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        try:
            with open(test_file_path, 'r') as f:
                test_samples = json.load(f)
            print(f"âœ… åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return test_samples
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
            return []
    
    def create_test_config(self, c_file_path: str, output_dir: str) -> tuple[str, str, bool]:
        """åˆ›å»ºæµ‹è¯•é…ç½® - åªå¤„ç†æœ‰å¯¹åº”æµ‹è¯•ç”¨ä¾‹çš„æ–‡ä»¶"""
        c_filename = os.path.basename(c_file_path)
        
        # æŸ¥æ‰¾å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆç°åœ¨è‚¯å®šå­˜åœ¨ï¼Œå› ä¸ºå·²ç»ç­›é€‰è¿‡äº†ï¼‰
        test_file_path = self.find_corresponding_test(c_file_path)
        
        if not test_file_path:
            raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹: {c_file_path}")
        
        # ä½¿ç”¨é¢„ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ - ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œä¸å¤åˆ¶
        print(f"ğŸ¯ ä½¿ç”¨é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {os.path.basename(test_file_path)}")
        test_samples_path = test_file_path  # ç›´æ¥ä½¿ç”¨åŸå§‹è·¯å¾„
        used_pregen = True
        
        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡æ–‡ä»¶
        test_task = []
        with open(test_samples_path, 'r') as f:
            test_samples = json.load(f)
        
        for i in range(len(test_samples)):
            test_task.append({
                "command": f"sactor run-tests --type bin {test_samples_path} %t {i} --feed-as-args",
                "test_id": i
            })
        
        test_task_path = os.path.join(output_dir, "test_task.json")
        with open(test_task_path, 'w') as f:
            json.dump(test_task, f, indent=2)
        
        return test_task_path, test_samples_path, used_pregen
    
    def estimate_api_cost(self, c_file_path: str, attempts: int) -> Dict:
        """ä¼°ç®—APIæˆæœ¬"""
        try:
            # è¯»å–Cæ–‡ä»¶å†…å®¹æ¥ä¼°ç®—tokenæ•°é‡
            with open(c_file_path, 'r') as f:
                c_content = f.read()
            
            # ç®€å•ä¼°ç®—ï¼šæ¯ä¸ªå­—ç¬¦çº¦ç­‰äº0.75ä¸ªtokenï¼ˆè‹±æ–‡ï¼‰
            input_tokens = len(c_content) * 0.75
            
            # ä¼°ç®—è¾“å‡ºï¼šå‡è®¾æ¯æ¬¡å°è¯•ç”Ÿæˆçº¦2å€è¾“å…¥é•¿åº¦çš„Rustä»£ç 
            output_tokens_per_attempt = input_tokens * 2
            
            # å¦‚æœattemptsä¸º0ï¼Œè¯´æ˜æ²¡æœ‰è¿›è¡Œç¿»è¯‘å°è¯•ï¼Œå¯èƒ½æ˜¯ç›´æ¥æˆåŠŸæˆ–å…¶ä»–åŸå› 
            # æ£€æŸ¥æ˜¯å¦æœ‰llm_stat.jsonæ–‡ä»¶æ¥è·å–çœŸå®çš„APIè°ƒç”¨ä¿¡æ¯
            # æ­£ç¡®çš„è·¯å¾„è½¬æ¢ï¼šä» raw_data è½¬æ¢åˆ° test ç›®å½•
            if '/raw_data/' in c_file_path:
                # ä» /home/changdi/sactor-datasets/Project_CodeNet/raw_data/argv/s997395205.c
                # è½¬æ¢ä¸º /home/changdi/sactor/test/argv/s997395205/llm_stat.json
                relative_path = c_file_path.replace('/home/changdi/sactor-datasets/Project_CodeNet/raw_data/', '')
                test_dir_path = os.path.join('/home/changdi/sactor/test', relative_path.replace('.c', ''))
                llm_stat_path = os.path.join(test_dir_path, "llm_stat.json")
            else:
                llm_stat_path = None
                
            if llm_stat_path and os.path.exists(llm_stat_path):
                try:
                    with open(llm_stat_path, 'r') as f:
                        llm_stat = json.load(f)
                    total_queries = llm_stat.get('total_queries', 0)
                    if total_queries > 0:
                        attempts = total_queries
                        print(f"ğŸ“Š ä»llm_stat.jsonè·å–åˆ°çœŸå®APIè°ƒç”¨æ¬¡æ•°: {attempts}")
                except Exception as e:
                    print(f"âš ï¸ è¯»å–llm_stat.jsonå¤±è´¥: {e}")
            
            if attempts == 0:
                # å‡è®¾è‡³å°‘è¿›è¡Œäº†ä¸€æ¬¡æˆåŠŸçš„ç¿»è¯‘
                attempts = 1
            
            # æ€»tokenä¼°ç®—
            total_input_tokens = input_tokens * attempts
            total_output_tokens = output_tokens_per_attempt * attempts
            
            # OpenAI GPT-4oå®šä»· (2025å¹´æœ€æ–°)
            # Input: $2.50 per 1M tokens = $0.0025 per 1K tokens
            # Output: $10.00 per 1M tokens = $0.01 per 1K tokens
            input_cost = (total_input_tokens / 1000) * 0.0025
            output_cost = (total_output_tokens / 1000) * 0.01
            total_cost = input_cost + output_cost
            
            return {
                'input_tokens': int(total_input_tokens),
                'output_tokens': int(total_output_tokens),
                'input_cost': input_cost,
                'output_cost': output_cost,
                'total_cost': total_cost,
                'cost_per_attempt': total_cost / attempts if attempts > 0 else total_cost,
                'estimated_attempts': attempts if attempts > 0 else 1
            }
            
        except Exception as e:
            return {
                'input_tokens': 0,
                'output_tokens': 0,
                'input_cost': 0,
                'output_cost': 0,
                'total_cost': 0,
                'cost_per_attempt': 0,
                'estimated_attempts': 0,
                'error': str(e)
            }
    
    def translate_with_sactor_docker(self, c_file_path: str, output_dir: str, test_task_path: str) -> Dict:
        """ä½¿ç”¨ SACToR Docker è¿›è¡Œç¿»è¯‘ - ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„"""
        try:
            # ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ŒæŒ‚è½½æ•´ä¸ªæ•°æ®é›†ç›®å½•
            sactor_config = "/home/changdi/sactor/sactor.toml"
            cmd = [
                "docker", "run", "--rm",
                "-v", f"{sactor_config}:/app/sactor.toml",
                "-v", f"/home/changdi/sactor-datasets:/home/changdi/sactor-datasets",
                "-v", f"{os.path.dirname(test_task_path)}:/tmp/test_tasks",
                "-v", f"{output_dir}:/tmp/result",
                "sactor", "translate",
                c_file_path,  # ç›´æ¥ä½¿ç”¨ç»å¯¹è·¯å¾„
                f"/tmp/test_tasks/{os.path.basename(test_task_path)}",
                "--result-dir", "/tmp/result",
                "--type", "bin"
            ]
            
            # ä½¿ç”¨è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´ï¼Œæ˜¾ç¤ºè¾“å‡º
            print(f"ğŸš€ æ‰§è¡ŒSACToR Dockerå‘½ä»¤...")
            result = subprocess.run(cmd, text=True, timeout=600)
            
            # ä¼°ç®—APIæˆæœ¬ï¼ˆä¼šå°è¯•ä»llm_stat.jsonè·å–çœŸå®attemptsï¼‰
            api_cost = self.estimate_api_cost(c_file_path, 0)
            attempts = api_cost.get('estimated_attempts', 0)
            
            # å°è¯•ä»è¾“å‡ºä¸­æå–å°è¯•æ¬¡æ•°ä¿¡æ¯ä½œä¸ºå¤‡ç”¨
            output_text = (result.stdout or "") + (result.stderr or "")
            
            # æŸ¥æ‰¾å°è¯•æ¬¡æ•°ç›¸å…³çš„ä¿¡æ¯
            if "Attempt" in output_text or "attempt" in output_text:
                output_attempts = output_text.lower().count("attempt")
                if output_attempts > attempts:
                    attempts = output_attempts
            
            # æŸ¥æ‰¾ MAX_ATTEMPTS_EXCEEDED é”™è¯¯
            if "MAX_ATTEMPTS_EXCEEDED" in output_text:
                attempts = 20  # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–çš„æœ€å¤§å°è¯•æ¬¡æ•°
                # é‡æ–°è®¡ç®—APIæˆæœ¬
                api_cost = self.estimate_api_cost(c_file_path, attempts)
            
            if result.returncode != 0:
                return {
                    'success': False,
                    'error': f"SACToR Docker ç¿»è¯‘å¤±è´¥: {result.stderr[:500]}...",
                    'result_dir': None,
                    'attempts': attempts,
                    'api_cost': api_cost
                }
            
            return {
                'success': True,
                'error': None,
                'result_dir': os.path.join(output_dir, "result"),
                'attempts': attempts,
                'api_cost': api_cost
            }
            
        except subprocess.TimeoutExpired:
            # è¶…æ—¶æ—¶ä¹Ÿä¼°ç®—æˆæœ¬
            api_cost = self.estimate_api_cost(c_file_path, 20)
            return {
                'success': False,
                'error': "SACToR Docker ç¿»è¯‘è¶…æ—¶ (10åˆ†é’Ÿ)",
                'result_dir': None,
                'attempts': 20,  # è¶…æ—¶é€šå¸¸æ„å‘³ç€è¾¾åˆ°äº†æœ€å¤§å°è¯•æ¬¡æ•°
                'api_cost': api_cost
            }
        except Exception as e:
            return {
                'success': False,
                'error': f"SACToR Docker ç¿»è¯‘å‡ºé”™: {e}",
                'result_dir': None,
                'attempts': 0,
                'api_cost': {'total_cost': 0, 'error': str(e)}
            }
    
    def verify_translation_result(self, result_dir: str) -> Dict:
        """éªŒè¯ç¿»è¯‘ç»“æœ"""
        try:
            verification_results = {
                'unidiomatic': {'success': False, 'details': {}},
                'idiomatic': {'success': False, 'details': {}},
                'overall': False,
                'test_count': 0
            }
            
            # æŸ¥æ‰¾ç¿»è¯‘ç»“æœ
            unidiomatic_dir = os.path.join(result_dir, "translated_code_unidiomatic")
            idiomatic_dir = os.path.join(result_dir, "translated_code_idiomatic")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¿»è¯‘ç»“æœ
            if os.path.exists(unidiomatic_dir):
                verification_results['unidiomatic'] = {'success': True, 'details': {'exists': True}}
            
            if os.path.exists(idiomatic_dir):
                verification_results['idiomatic'] = {'success': True, 'details': {'exists': True}}
            
            # è®¡ç®—æµ‹è¯•æ•°é‡ - ä»å¤šä¸ªå¯èƒ½çš„ä½ç½®æŸ¥æ‰¾test_samples.json
            test_count = 0
            possible_test_paths = [
                os.path.join(result_dir, "..", "test_samples.json"),
                os.path.join(result_dir, "test_samples.json"),
                os.path.join(os.path.dirname(result_dir), "test_samples.json")
            ]
            
            for test_samples_path in possible_test_paths:
                if os.path.exists(test_samples_path):
                    try:
                        with open(test_samples_path, 'r') as f:
                            test_samples = json.load(f)
                        # è¿‡æ»¤æ‰ç©ºçš„æµ‹è¯•ç”¨ä¾‹
                        if isinstance(test_samples, list):
                            test_count = len([t for t in test_samples if t])  # è¿‡æ»¤ç©ºå…ƒç´ 
                        else:
                            test_count = len(test_samples) if test_samples else 0
                        break
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–æµ‹è¯•ç”¨ä¾‹å¤±è´¥ {test_samples_path}: {e}")
                        continue
            
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»test_task.jsonæ¨æ–­
            if test_count == 0:
                test_task_path = os.path.join(result_dir, "..", "test_task.json")
                if os.path.exists(test_task_path):
                    try:
                        with open(test_task_path, 'r') as f:
                            test_tasks = json.load(f)
                        if isinstance(test_tasks, list):
                            test_count = len([t for t in test_tasks if t])  # è¿‡æ»¤ç©ºå…ƒç´ 
                        else:
                            test_count = len(test_tasks) if test_tasks else 0
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–æµ‹è¯•ä»»åŠ¡å¤±è´¥ {test_task_path}: {e}")
            
            verification_results['test_count'] = test_count
            
            # ç»¼åˆç»“æœ
            verification_results['overall'] = (
                verification_results['unidiomatic']['success'] and 
                verification_results['idiomatic']['success']
            )
            
            return verification_results
            
        except Exception as e:
            return {
                'unidiomatic': {'success': False, 'error': str(e)},
                'idiomatic': {'success': False, 'error': str(e)},
                'overall': False,
                'test_count': 0
            }
    
    def translate_and_verify(self, c_file_path: str, output_dir: str) -> Dict:
        """ç¿»è¯‘å’ŒéªŒè¯å•ä¸ª C æ–‡ä»¶"""
        start_time = time.time()
        try:
            print(f"ğŸ¯ æ­£ç¡®æ•°æ®ç¿»è¯‘: {os.path.basename(c_file_path)}")
            
            # 1. åˆ›å»ºæµ‹è¯•é…ç½®
            test_task_path, test_samples_path, used_pregen = self.create_test_config(c_file_path, output_dir)
            
            # 2. ä½¿ç”¨ SACToR Docker è¿›è¡Œç¿»è¯‘
            translation_result = self.translate_with_sactor_docker(c_file_path, output_dir, test_task_path)
            
            if not translation_result['success']:
                return {
                    'success': False,
                    'error': translation_result['error'],
                    'verification': None,
                    'test_count': 0,
                    'used_pregen': used_pregen,
                    'processing_time': time.time() - start_time,
                    'attempts': translation_result.get('attempts', 0),
                    'api_cost': translation_result.get('api_cost', {})
                }
            
            # 3. éªŒè¯ç¿»è¯‘ç»“æœ
            verification_results = self.verify_translation_result(translation_result['result_dir'])
            
            return {
                'success': True,
                'error': None,
                'verification': verification_results,
                'test_count': verification_results.get('test_count', 0),
                'result_dir': translation_result['result_dir'],
                'used_pregen': used_pregen,
                'processing_time': time.time() - start_time,
                'attempts': translation_result.get('attempts', 0),
                'api_cost': translation_result.get('api_cost', {})
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f"ç¿»è¯‘å¤±è´¥: {e}",
                'verification': None,
                'test_count': 0,
                'used_pregen': False,
                'processing_time': time.time() - start_time,
                'attempts': 0,
                'api_cost': {'total_cost': 0, 'error': str(e)}
            }
    
    def batch_translate(self, output_base_dir: str, max_files: int = None) -> Dict:
        """æ‰¹é‡ç¿»è¯‘ - åªå¤„ç†æœ‰å¯¹åº”æµ‹è¯•ç”¨ä¾‹çš„ C æ–‡ä»¶"""
        all_c_files = []
        skipped_files = []
        
        # æ”¶é›†æœ‰å¯¹åº”æµ‹è¯•ç”¨ä¾‹çš„ C æ–‡ä»¶
        for subdir in ["argv", "scanf"]:
            subdir_path = os.path.join(self.raw_data_dir, subdir)
            if os.path.exists(subdir_path):
                c_files = [os.path.join(subdir_path, f) for f in os.listdir(subdir_path) if f.endswith('.c')]
                
                for c_file in c_files:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æµ‹è¯•ç”¨ä¾‹
                    test_file = self.find_corresponding_test(c_file)
                    if test_file:
                        all_c_files.append(c_file)
                    else:
                        skipped_files.append(c_file)
        
        print(f"ğŸ¯ ç­›é€‰ç»“æœ:")
        print(f"   - æœ‰æµ‹è¯•ç”¨ä¾‹çš„ C æ–‡ä»¶: {len(all_c_files)} ä¸ª")
        print(f"   - è·³è¿‡æ²¡æœ‰æµ‹è¯•ç”¨ä¾‹çš„æ–‡ä»¶: {len(skipped_files)} ä¸ª")
        
        # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if max_files is not None and len(all_c_files) > max_files:
            all_c_files = all_c_files[:max_files]
            print(f"âš ï¸ é™åˆ¶å¤„ç†å‰ {max_files} ä¸ªæ–‡ä»¶")
        
        total_files = len(all_c_files)
        print(f"ğŸš€ å¼€å§‹æ­£ç¡®æ•°æ®æ‰¹é‡ç¿»è¯‘ {total_files} ä¸ª C æ–‡ä»¶")
        
        results = {
            'total': total_files,
            'success': 0,
            'failed': 0,
            'verified': 0,
            'skipped': len(skipped_files),
            'total_processing_time': 0,
            'total_attempts': 0,
            'total_api_cost': 0,
            'avg_processing_time': 0,
            'avg_attempts': 0,
            'avg_api_cost': 0,
            'details': [],
            'start_time': time.time()
        }
        
        for i, c_file_path in enumerate(all_c_files):
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i+1}/{total_files}: {os.path.basename(c_file_path)}")
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºè¾“å‡ºç›®å½•
            relative_path = os.path.relpath(c_file_path, self.raw_data_dir)
            file_output_dir = os.path.join(output_base_dir, relative_path.replace('.c', ''))
            os.makedirs(file_output_dir, exist_ok=True)
            
            # ç¿»è¯‘å’ŒéªŒè¯
            result = self.translate_and_verify(c_file_path, file_output_dir)
            
            # è®°å½•ç¿»è¯‘ç»“æœåˆ°æ—¥å¿—
            self.log_translation_result(c_file_path, result, output_base_dir)
            
            if result['success']:
                results['success'] += 1
                if result['verification'] and result['verification']['overall']:
                    results['verified'] += 1
            
            # ç»Ÿè®¡å¤„ç†æ—¶é—´ã€å°è¯•æ¬¡æ•°å’ŒAPIæˆæœ¬
            processing_time = result.get('processing_time', 0)
            attempts = result.get('attempts', 0)
            api_cost = result.get('api_cost', {})
            total_cost = api_cost.get('total_cost', 0)
            
            results['total_processing_time'] += processing_time
            results['total_attempts'] += attempts
            results['total_api_cost'] += total_cost
            
            results['details'].append({
                'file': os.path.basename(c_file_path),
                'directory': os.path.dirname(c_file_path),
                'success': result['success'],
                'verified': result['verification']['overall'] if result['verification'] else False,
                'test_count': result['test_count'],
                'processing_time': processing_time,
                'attempts': attempts,
                'api_cost': api_cost,
                'error': result['error']
            })
            
            results['failed'] = results['total'] - results['success']
            
            # æ¯å¤„ç† 10 ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                self._save_progress(results, output_base_dir, i + 1)
                print(f"ğŸ“ˆ è¿›åº¦ç»Ÿè®¡: å¤„ç†æ—¶é—´ {processing_time:.1f}s, å°è¯•æ¬¡æ•° {attempts}, APIæˆæœ¬ ${total_cost:.4f}")
        
        results['end_time'] = time.time()
        results['duration'] = results['end_time'] - results['start_time']
        
        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´ã€å¹³å‡å°è¯•æ¬¡æ•°å’Œå¹³å‡APIæˆæœ¬
        if results['total'] > 0:
            results['avg_processing_time'] = results['total_processing_time'] / results['total']
            results['avg_attempts'] = results['total_attempts'] / results['total']
            results['avg_api_cost'] = results['total_api_cost'] / results['total']
        
        return results
    
    def _save_progress(self, results: Dict, output_base_dir: str, processed_count: int):
        """ä¿å­˜è¿›åº¦"""
        progress_file = os.path.join(output_base_dir, f"progress_{processed_count}.json")
        with open(progress_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {processed_count}/{results['total']} æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    # è·å–æ¨¡å‹ä¿¡æ¯
    translator = CorrectDataTranslator()
    model_name = translator._get_model_info().replace('-', '_').replace('.', '_')
    
    # é…ç½®è·¯å¾„ - åŒ…å«æ¨¡å‹åç§°
    output_base_dir = f"/home/changdi/sactor/test_{model_name}"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_base_dir, exist_ok=True)
    
    # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°å’Œé¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹è¦†ç›–
    raw_data_dir = "/home/changdi/sactor-datasets/Project_CodeNet/raw_data"
    test_data_dir = "/home/changdi/sactor-datasets/Project_CodeNet/generated_tests"
    
    argv_c_files = len(os.listdir(os.path.join(raw_data_dir, "argv")))
    scanf_c_files = len(os.listdir(os.path.join(raw_data_dir, "scanf")))
    total_c_files = argv_c_files + scanf_c_files
    
    argv_test_files = len(os.listdir(os.path.join(test_data_dir, "argv")))
    scanf_test_files = len(os.listdir(os.path.join(test_data_dir, "scanf")))
    total_test_files = argv_test_files + scanf_test_files
    
    print(f"ğŸ“ åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   - argv: {argv_c_files} ä¸ª C æ–‡ä»¶")
    print(f"   - scanf: {scanf_c_files} ä¸ª C æ–‡ä»¶")
    print(f"   - æ€»è®¡: {total_c_files} ä¸ª C æ–‡ä»¶")
    print(f"ğŸ¯ é¢„ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ç»Ÿè®¡:")
    print(f"   - argv: {argv_test_files} ä¸ªæµ‹è¯•æ–‡ä»¶")
    print(f"   - scanf: {scanf_test_files} ä¸ªæµ‹è¯•æ–‡ä»¶")
    print(f"   - æ€»è®¡: {total_test_files} ä¸ªæµ‹è¯•æ–‡ä»¶")
    print(f"ğŸ“Š æµ‹è¯•ç”¨ä¾‹è¦†ç›–ç‡: {total_test_files}/{total_c_files} = {total_test_files/total_c_files*100:.1f}%")
    
    # åˆ›å»ºç¿»è¯‘å™¨
    translator = CorrectDataTranslator()
    
    try:
        # æ‰¹é‡ç¿»è¯‘
        results = translator.batch_translate(output_base_dir, max_files=None)
        
        # è¾“å‡ºç»“æœç»Ÿè®¡
        print(f"\nğŸ“Š SACToR æ‰¹é‡ç¿»è¯‘ç»“æœ (ä»…å¤„ç†æœ‰æµ‹è¯•ç”¨ä¾‹çš„æ–‡ä»¶):")
        print(f"å¤„ç†æ–‡ä»¶æ•°: {results['total']}")
        print(f"è·³è¿‡æ–‡ä»¶æ•°: {results['skipped']}")
        print(f"ç¿»è¯‘æˆåŠŸ: {results['success']}")
        print(f"ç¿»è¯‘å¤±è´¥: {results['failed']}")
        print(f"éªŒè¯é€šè¿‡: {results['verified']}")
        print(f"æ€»å¤„ç†æ—¶é—´: {results['duration']:.2f} ç§’")
        print(f"æ€»å°è¯•æ¬¡æ•°: {results['total_attempts']}")
        print(f"æ€»APIæˆæœ¬: ${results['total_api_cost']:.4f}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {results['avg_processing_time']:.2f} ç§’/æ–‡ä»¶")
        print(f"å¹³å‡å°è¯•æ¬¡æ•°: {results['avg_attempts']:.1f} æ¬¡/æ–‡ä»¶")
        print(f"å¹³å‡APIæˆæœ¬: ${results['avg_api_cost']:.4f}/æ–‡ä»¶")
        print(f"æˆåŠŸç‡: {results['success']/results['total']*100:.1f}%")
        print(f"éªŒè¯ç‡: {results['verified']/results['total']*100:.1f}%")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(output_base_dir, "sactor_correct_results.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return results
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if hasattr(translator, 'temp_dir') and os.path.exists(translator.temp_dir):
            shutil.rmtree(translator.temp_dir)

if __name__ == "__main__":
    main()
