# vLLM é›†æˆæ€»ç»“æ–‡æ¡£ (2024-11-17)

## ğŸ“‹ ç›®å½•

1. [åŠŸèƒ½æ¦‚è¿°](#åŠŸèƒ½æ¦‚è¿°)
2. [å®ç°å†…å®¹](#å®ç°å†…å®¹)
3. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
4. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
5. [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
7. [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)

---

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

### ä¸»è¦åŠŸèƒ½

SACToR ç°å·²æ”¯æŒä½¿ç”¨ **vLLM** æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹è¿›è¡Œ Câ†’Rust ä»£ç ç¿»è¯‘ï¼Œæ›¿ä»£åŸæœ‰çš„ API æ¨¡å‹è°ƒç”¨ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

- âœ… **å®Œå…¨æœ¬åœ°è¿è¡Œ**ï¼šæ•°æ®ä¸ä¸Šä¼ ï¼Œéšç§æœ‰ä¿éšœ
- âœ… **æ—  API è´¹ç”¨**ï¼šæœ¬åœ°éƒ¨ç½²ï¼Œæ— è°ƒç”¨æˆæœ¬
- âœ… **æ— é…é¢é™åˆ¶**ï¼šä¸å— API é…é¢é™åˆ¶
- âœ… **OpenAI å…¼å®¹**ï¼šä½¿ç”¨ OpenAI å…¼å®¹çš„ API æ¥å£
- âœ… **çµæ´»é…ç½®**ï¼šæ”¯æŒè‡ªå®šä¹‰ç«¯å£ã€æ¨¡å‹ã€å‚æ•°ç­‰

### é€‚ç”¨åœºæ™¯

- å¤§è§„æ¨¡ä»£ç ç¿»è¯‘ä»»åŠ¡
- éœ€è¦æ•°æ®éšç§ä¿æŠ¤çš„åœºæ™¯
- å¸Œæœ›é™ä½ API è°ƒç”¨æˆæœ¬çš„åœºæ™¯
- éœ€è¦é•¿æ—¶é—´è¿è¡Œçš„ç¿»è¯‘ä»»åŠ¡

---

## ğŸ“¦ å®ç°å†…å®¹

### 1. æ ¸å¿ƒå®ç°æ–‡ä»¶

#### `sactor/llm/vllm_llm.py`
- **åŠŸèƒ½**ï¼švLLM LLM åŒ…è£…å™¨å®ç°
- **ç‰¹ç‚¹**ï¼š
  - å®ç° OpenAI å…¼å®¹ API æ¥å£
  - æ­£ç¡®å¤„ç† Python 3.9+ å…¼å®¹æ€§ï¼ˆoverride è£…é¥°å™¨ï¼‰
  - æ”¯æŒè‡ªå®šä¹‰ base_urlã€modelã€temperature ç­‰å‚æ•°
  - å®Œæ•´çš„é”™è¯¯å¤„ç†

#### `sactor/llm/__init__.py`
- **åŠŸèƒ½**ï¼šLLM å·¥å‚å‡½æ•°æ›´æ–°
- **ä¿®æ”¹**ï¼š
  - æ·»åŠ  VLLMLLM å¯¼å…¥
  - åœ¨ `llm_factory()` ä¸­æ·»åŠ  "VLLM" case
  - æ›´æ–° `__all__` åˆ—è¡¨

### 2. é…ç½®æ–‡ä»¶

#### `sactor.default.toml` / `sactor.toml`
- **æ–°å¢é…ç½®æ®µ**ï¼š`[VLLM]`
- **é…ç½®é¡¹**ï¼š
  ```toml
  [VLLM]
  base_url = "http://localhost:8000/v1"
  api_key = "EMPTY"
  model = "Qwen/Qwen2.5-1.5B-Instruct"
  max_tokens = 2048
  temperature = 0.7
  ```

### 3. å·¥å…·è„šæœ¬

#### `start_vllm.sh`
- **åŠŸèƒ½**ï¼šå¿«é€Ÿå¯åŠ¨ vLLM æœåŠ¡
- **ç”¨æ³•**ï¼š
  ```bash
  ./start_vllm.sh
  # æˆ–è‡ªå®šä¹‰å‚æ•°
  MODEL=Qwen/Qwen2.5-7B-Instruct PORT=8001 ./start_vllm.sh
  ```

#### `test_vllm_standalone.py`
- **åŠŸèƒ½**ï¼šç‹¬ç«‹æµ‹è¯• vLLM è¿æ¥ï¼ˆä¸ä¾èµ–æ•´ä¸ªé¡¹ç›®ï¼‰
- **ç”¨æ³•**ï¼š
  ```bash
  python3 test_vllm_standalone.py
  ```

#### `test_vllm_translate_simple.py`
- **åŠŸèƒ½**ï¼šæµ‹è¯• vLLM ç¿»è¯‘åŠŸèƒ½ï¼ˆä¸ä¾èµ– c2rust/crownï¼‰
- **ç”¨æ³•**ï¼š
  ```bash
  python3 test_vllm_translate_simple.py
  ```

### 4. æ–‡æ¡£æ–‡ä»¶

- `VLLM_USAGE.md` - å®Œæ•´ä½¿ç”¨æ–‡æ¡£
- `VLLM_QUICKSTART.md` - å¿«é€Ÿå¼€å§‹æŒ‡å—
- `VLLM_INTEGRATION_SUMMARY.md` - é›†æˆæ€»ç»“

---

## âš™ï¸ é…ç½®è¯´æ˜

### 1. åŸºæœ¬é…ç½®

ç¼–è¾‘ `sactor.toml`ï¼š

```toml
[general]
llm = "VLLM"  # è®¾ç½®ä¸º VLLM

[VLLM]
base_url = "http://localhost:8000/v1"
api_key = "EMPTY"
model = "Qwen/Qwen2.5-1.5B-Instruct"
max_tokens = 2048
temperature = 0.7
```

### 2. é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¤‡æ³¨ |
|------|------|--------|------|
| `base_url` | vLLM æœåŠ¡åœ°å€ | `http://localhost:8000/v1` | å¿…é¡»åŒ…å« `/v1` |
| `api_key` | API å¯†é’¥ | `EMPTY` | vLLM ä¸éœ€è¦çœŸå®å¯†é’¥ |
| `model` | æ¨¡å‹åç§° | `Qwen/Qwen2.5-1.5B-Instruct` | éœ€ä¸ vLLM æœåŠ¡å¯åŠ¨æ—¶ä¸€è‡´ |
| `max_tokens` | æœ€å¤§ç”Ÿæˆ token æ•° | `2048` | éœ€å°äºæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ |
| `temperature` | ç”Ÿæˆæ¸©åº¦ | `0.7` | æ§åˆ¶éšæœºæ€§ |

### 3. æ¨¡å‹é€‰æ‹©å»ºè®®

| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|----------|
| Qwen/Qwen2.5-1.5B-Instruct | 1.5B | ~4GB VRAM | æµ‹è¯•ã€å¿«é€ŸéªŒè¯ |
| Qwen/Qwen2.5-7B-Instruct | 7B | ~16GB VRAM | ç”Ÿäº§ç¯å¢ƒã€é«˜è´¨é‡ç¿»è¯‘ |
| Qwen/Qwen2.5-14B-Instruct | 14B | ~32GB VRAM | å¤æ‚ä»£ç ç¿»è¯‘ |

**æ³¨æ„**ï¼š`max_tokens` éœ€è¦æ ¹æ®æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦è°ƒæ•´ï¼š
- 1.5B: 4096 tokens â†’ max_tokens â‰¤ 2048
- 7B: 8192 tokens â†’ max_tokens â‰¤ 4096
- 14B: 16384 tokens â†’ max_tokens â‰¤ 8192

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ­¥éª¤ 1: å®‰è£… vLLM

```bash
# ä½¿ç”¨ pip å®‰è£…
pip install vllm

# æˆ–ä½¿ç”¨ conda
conda install -c conda-forge vllm
```

### æ­¥éª¤ 2: å¯åŠ¨ vLLM æœåŠ¡

#### æ–¹æ³• 1: ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /home/changdi/sactor
./start_vllm.sh
```

#### æ–¹æ³• 2: æ‰‹åŠ¨å¯åŠ¨

```bash
# ä½¿ç”¨ Qwen 1.5B æ¨¡å‹ï¼ˆæµ‹è¯•ç”¨ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-1.5B-Instruct \
  --port 8000 \
  --trust-remote-code

# ä½¿ç”¨ Qwen 7B æ¨¡å‹ï¼ˆç”Ÿäº§ç”¨ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --port 8000 \
  --trust-remote-code \
  --gpu-memory-utilization 0.9
```

#### æ–¹æ³• 3: è‡ªå®šä¹‰ç«¯å£

```bash
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-1.5B-Instruct \
  --port 8001 \
  --trust-remote-code
```

**æ³¨æ„**ï¼šå¦‚æœä½¿ç”¨è‡ªå®šä¹‰ç«¯å£ï¼Œéœ€è¦åœ¨ `sactor.toml` ä¸­ä¿®æ”¹ `base_url`ã€‚

### æ­¥éª¤ 3: é…ç½® SACToR

ç¼–è¾‘ `sactor.toml`ï¼š

```toml
[general]
llm = "VLLM"

[VLLM]
base_url = "http://localhost:8000/v1"
api_key = "EMPTY"
model = "Qwen/Qwen2.5-1.5B-Instruct"
max_tokens = 2048
temperature = 0.7
```

### æ­¥éª¤ 4: éªŒè¯è¿æ¥

```bash
# æ–¹æ³• 1: ä½¿ç”¨ç‹¬ç«‹æµ‹è¯•è„šæœ¬ï¼ˆæ¨èï¼‰
python3 test_vllm_standalone.py

# æ–¹æ³• 2: ä½¿ç”¨ç®€å•ç¿»è¯‘æµ‹è¯•
source .venv/bin/activate
python3 test_vllm_translate_simple.py

# æ–¹æ³• 3: æ‰‹åŠ¨æµ‹è¯• API
curl http://localhost:8000/v1/models
```

### æ­¥éª¤ 5: è¿è¡Œç¿»è¯‘

#### å•æ–‡ä»¶ç¿»è¯‘

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# è¿è¡Œç¿»è¯‘
sactor translate \
  /path/to/file.c \
  /path/to/test_task.json \
  -r /path/to/result \
  --type bin
```

#### æ‰¹é‡ç¿»è¯‘

```bash
python3 batch_translate_generic.py \
    --c-files /path/to/c_files \
    --json-files /path/to/json_files \
    --output /path/to/output \
    --workers 4 \
    --num-tests 6
```

---

## âœ… æµ‹è¯•éªŒè¯

### æµ‹è¯• 1: è¿æ¥æµ‹è¯•

```bash
python3 test_vllm_standalone.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… æˆåŠŸè¿æ¥åˆ° vLLM æœåŠ¡
   å¯ç”¨æ¨¡å‹æ•°: 1
   - Qwen/Qwen2.5-1.5B-Instruct
âœ… æŸ¥è¯¢æˆåŠŸ!
```

### æµ‹è¯• 2: ç¿»è¯‘åŠŸèƒ½æµ‹è¯•

```bash
source .venv/bin/activate
python3 test_vllm_translate_simple.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… LLM å®ä¾‹åˆ›å»ºæˆåŠŸ: VLLMLLM
âœ… ç¿»è¯‘æˆåŠŸ!
âœ… æ£€æµ‹åˆ° Rust å…³é”®å­—: fn , ->, i32
```

### æµ‹è¯• 3: å®Œæ•´ç¿»è¯‘æµ‹è¯•

```bash
source .venv/bin/activate
cd tests/c_examples/atoi
sactor translate atoi.c test_task/test_task.json -r result_vllm --type bin
```

**é¢„æœŸç»“æœ**ï¼š
- ç”Ÿæˆ `result_vllm/translated_code_unidiomatic/combined.rs`
- ç”Ÿæˆ `result_vllm/translated_code_idiomatic/combined.rs`
- ç¼–è¯‘å’Œæµ‹è¯•é€šè¿‡

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1: è¿æ¥å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
Connection error.
âŒ æ— æ³•åˆ—å‡ºæ¨¡å‹
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥ vLLM æœåŠ¡æ˜¯å¦è¿è¡Œï¼š
   ```bash
   ps aux | grep vllm
   curl http://localhost:8000/v1/models
   ```

2. æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®ï¼š
   ```bash
   netstat -tuln | grep 8000
   ```

3. ç¡®è®¤ `base_url` é…ç½®æ­£ç¡®ï¼ˆå¿…é¡»åŒ…å« `/v1`ï¼‰

### é—®é¢˜ 2: æ¨¡å‹åŠ è½½å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
Model not found
Failed to load model
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç¡®è®¤æ¨¡å‹åç§°æ­£ç¡®ï¼ˆä¸ HuggingFace ä¸Šçš„åç§°ä¸€è‡´ï¼‰
2. é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸
3. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³

### é—®é¢˜ 3: max_tokens é”™è¯¯

**ç—‡çŠ¶**ï¼š
```
Error code: 400 - 'max_tokens' is too large: 8192
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼š
   ```bash
   curl http://localhost:8000/v1/models | grep max_model_len
   ```

2. è°ƒæ•´ `max_tokens` é…ç½®ï¼š
   ```toml
   [VLLM]
   max_tokens = 2048  # å°äºæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„ä¸€åŠ
   ```

### é—®é¢˜ 4: GPU å†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡ï¼š
   ```bash
   python -m vllm.entrypoints.openai.api_server \
     --model Qwen/Qwen2.5-1.5B-Instruct \
     --gpu-memory-utilization 0.7
   ```

2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
3. å‡å°‘ `max-model-len` å‚æ•°

### é—®é¢˜ 5: ç¿»è¯‘è´¨é‡å·®

**å¯èƒ½åŸå› **ï¼š
- æ¨¡å‹å¤ªå°ï¼ˆ1.5B å¯èƒ½ä¸å¤Ÿï¼‰
- temperature è®¾ç½®ä¸åˆé€‚
- max_tokens å¤ªå°å¯¼è‡´æˆªæ–­

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆ7B æˆ–æ›´å¤§ï¼‰
2. è°ƒæ•´ temperatureï¼ˆ0.5-0.9ï¼‰
3. å¢åŠ  max_tokensï¼ˆåœ¨æ¨¡å‹é™åˆ¶å†…ï¼‰

---

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### è°ƒç”¨æµç¨‹

```
sactor.toml (llm = "VLLM")
    â†“
llm_factory(config)
    â†“
æ£€æŸ¥ llm = "VLLM"
    â†“
åˆ›å»º VLLMLLM å®ä¾‹
    â†“
llm.query(prompt)
    â†“
VLLMLLM._query_impl()
    â†“
OpenAI å®¢æˆ·ç«¯ (base_url=http://localhost:8000/v1)
    â†“
vLLM æœåŠ¡ API
    â†“
è¿”å›ç¿»è¯‘ç»“æœ
```

### å…³é”®ä»£ç ä½ç½®

1. **LLM å·¥å‚å‡½æ•°**ï¼š`sactor/llm/__init__.py:40-41`
   ```python
   case "VLLM":
       return VLLMLLM(config, encoding=encoding, system_msg=system_message)
   ```

2. **VLLM å®ç°**ï¼š`sactor/llm/vllm_llm.py:70`
   ```python
   resp = self.client.chat.completions.create(
       model=model,
       messages=messages,
       temperature=temperature,
       max_tokens=max_tokens,
   )
   ```

3. **Sactor åˆå§‹åŒ–**ï¼š`sactor/sactor.py:93`
   ```python
   self.llm = llm_factory(self.config)
   ```

### å…¼å®¹æ€§è¯´æ˜

- **Python ç‰ˆæœ¬**ï¼šæ”¯æŒ Python 3.9+
- **vLLM ç‰ˆæœ¬**ï¼šå»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
- **OpenAI å®¢æˆ·ç«¯**ï¼šä½¿ç”¨ `openai` åº“çš„ OpenAI å…¼å®¹æ¥å£

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **GPU è®¾ç½®**ï¼š
   - ä½¿ç”¨ `--gpu-memory-utilization 0.9` æœ€å¤§åŒ– GPU ä½¿ç”¨
   - å¤š GPU ä½¿ç”¨ `--tensor-parallel-size 2`

2. **å¹¶å‘è®¾ç½®**ï¼š
   - æ‰¹é‡ç¿»è¯‘æ—¶ï¼Œ`--workers` å»ºè®®è®¾ç½®ä¸º 4-10
   - æ ¹æ® GPU å†…å­˜è°ƒæ•´å¹¶å‘æ•°

3. **æ¨¡å‹é€‰æ‹©**ï¼š
   - æµ‹è¯•ï¼š1.5Bï¼ˆå¿«é€Ÿï¼‰
   - ç”Ÿäº§ï¼š7B+ï¼ˆè´¨é‡ï¼‰

---

## ğŸ“ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

### vLLM æœåŠ¡ç®¡ç†

```bash
# å¯åŠ¨æœåŠ¡
./start_vllm.sh

# å¯åŠ¨æœåŠ¡ï¼ˆè‡ªå®šä¹‰æ¨¡å‹ï¼‰
MODEL=Qwen/Qwen2.5-7B-Instruct ./start_vllm.sh

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/v1/models

# åœæ­¢æœåŠ¡
pkill -f vllm
```

### SACToR ç¿»è¯‘å‘½ä»¤

```bash
# æ¿€æ´»ç¯å¢ƒ
source .venv/bin/activate

# å•æ–‡ä»¶ç¿»è¯‘
sactor translate file.c test_task.json -r result --type bin

# æ‰¹é‡ç¿»è¯‘
python3 batch_translate_generic.py \
    --c-files /path/to/c_files \
    --json-files /path/to/json_files \
    --output /path/to/output \
    --workers 4
```

### æµ‹è¯•å‘½ä»¤

```bash
# è¿æ¥æµ‹è¯•
python3 test_vllm_standalone.py

# ç¿»è¯‘åŠŸèƒ½æµ‹è¯•
python3 test_vllm_translate_simple.py

# å®Œæ•´ç¿»è¯‘æµ‹è¯•
sactor translate tests/c_examples/atoi/atoi.c \
    tests/c_examples/atoi/test_task/test_task.json \
    -r tests/c_examples/atoi/result_vllm \
    --type bin
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `VLLM_USAGE.md` - è¯¦ç»†ä½¿ç”¨æ–‡æ¡£
- `VLLM_QUICKSTART.md` - å¿«é€Ÿå¼€å§‹æŒ‡å—
- `README.md` - é¡¹ç›®ä¸»æ–‡æ¡£
- `TRANSLATION_GUIDE.md` - ç¿»è¯‘æŒ‡å—

---

## ğŸ‰ æ€»ç»“

vLLM é›†æˆå·²å®Œæˆå¹¶é€šè¿‡æµ‹è¯•ï¼Œä¸»è¦ç‰¹ç‚¹ï¼š

- âœ… **å®Œå…¨é›†æˆ**ï¼šæ— ç¼æ›¿ä»£ API æ¨¡å‹
- âœ… **é…ç½®ç®€å•**ï¼šåªéœ€ä¿®æ”¹ `sactor.toml`
- âœ… **æµ‹è¯•å®Œå–„**ï¼šæä¾›å¤šä¸ªæµ‹è¯•è„šæœ¬
- âœ… **æ–‡æ¡£é½å…¨**ï¼šåŒ…å«å®Œæ•´çš„ä½¿ç”¨æ–‡æ¡£

**ä¸‹ä¸€æ­¥**ï¼š
1. æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹
2. å¯åŠ¨ vLLM æœåŠ¡
3. é…ç½® `sactor.toml`
4. å¼€å§‹ç¿»è¯‘ï¼

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**æ›´æ–°æ—¥æœŸ**ï¼š2024-11-17  
**ä½œè€…**ï¼šSACToR Team

