[general]
llm = "OpenAI" # 可选: OpenAI, AzureOpenAI, DeepSeek, Anthropic, Google, Ollama, Qwen
max_translation_attempts = 20
max_verifier_harness_attempts = 6
timeout_seconds = 60 # timeout for the execution of generated code
system_message = '''
You are an expert in translating code from C to Rust. You will take all information from the user as reference, and will output the translated code into the format that the user wants.
'''
encoding = "o200k_base" # Encoding for the `tiktoken` library, default for GPT-4o model

[AzureOpenAI]
api_key = "your-api-key"
endpoint = "https://your-endpoint.openai.azure.com/"
api_version = "2024-12-26" # change this to your own API version
model = "gpt-4o" # change this to your own model
max_tokens = 8192 # 用于非GPT-5模型
max_completion_tokens = 8192 # 用于GPT-5模型
# temperature = 1 # Uncomment this line if you want to set a temperature

[OpenAI]
api_key = "xxx"
model = "gpt-5" # 可选: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, gpt-5
max_tokens = 8192 # 用于非GPT-5模型
max_completion_tokens = 8192 # 用于GPT-5模型
# temperature = 1 # Uncomment this line if you want to set a temperature
# organization = "your-organization" # uncomment this line if you are using an organization
# project_id = "your-project-id" # uncomment this line if you are using a project id
# base_url = "https://api.openai.com/v1" # uncomment this line if you are using a custom base url

[DeepSeek]
base_url = "https://api.deepseek.com"
api_key = "sk-your-deepseek-api-key"
model = "deeepseek-reasoner" # change this to your own model
max_tokens = 8192 # maximum number of tokens to generate for each request
# temperature = 1 # Uncomment this line if you want to set a temperature
# organization = "your-organization" # uncomment this line if you are using an organization
# project_id = "your-project-id" # uncomment this line if you are using a project id

[Anthropic]
api_key = "sk-ant-your-anthropic-api-key"
model = "claude-3-5-sonnet-latest" # change this to your own model
max_tokens = 8192 # maximum number of tokens to generate for each request
# temperature  = 1 # Uncomment this line if you want to set a temperature

[Google]
api_key = "your-api-key"
model = 'gemini-2.0-flash-exp' # change this to your own model
max_tokens = 8192 # maximum number of tokens to generate for each request
# temperature = 1 # Uncomment this line if you want to set a temperature

[Ollama]
host = "http://127.0.0.1:11434"
model = "llama3.3"
max_tokens = 16384 # maximum number of tokens for each request
# temperature = 0.8 # Uncomment this line if you want to set a temperature

[Ollama.headers]
# Add any headers you want to send with the request
# Authorization = "Bearer your-token"

[Qwen]
base_url = "https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
api_key = "xx"
model = "qwen3-coder-plus-2025-09-23" # 可选: qwen-plus, qwen-max, qwen-turbo, qwen-long
max_tokens = 8192 # maximum number of tokens to generate for each request
max_completion_tokens = 8192 # 用于支持max_completion_tokens的模型
# temperature = 1 # Uncomment this line if you want to set a temperature

[test_generator]
max_attempts = 6
timeout_seconds = 60

[test_runner]
timeout_seconds = 60
