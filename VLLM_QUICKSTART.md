# vLLM å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3 æ­¥ï¼‰

### æ­¥éª¤ 1: å®‰è£… vLLM

```bash
pip install vllm
```

### æ­¥éª¤ 2: å¯åŠ¨ vLLM æœåŠ¡

```bash
# ä½¿ç”¨ Qwen 1.5B æ¨¡å‹ï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-1.5B-Instruct \
  --port 8000 \
  --trust-remote-code
```

æˆ–è€…ä½¿ç”¨æä¾›çš„è„šæœ¬ï¼š

```bash
./start_vllm.sh
```

### æ­¥éª¤ 3: é…ç½® SACToR

ç¼–è¾‘ `sactor.toml`ï¼š

```toml
[general]
llm = "VLLM"

[VLLM]
base_url = "http://localhost:8000/v1"
api_key = "EMPTY"
model = "Qwen/Qwen2.5-1.5B-Instruct"
max_tokens = 8192
temperature = 0.7
```

## âœ… éªŒè¯å®‰è£…

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
python3 test_vllm.py
```

æˆ–è€…æ‰‹åŠ¨æµ‹è¯•ï¼š

```bash
curl http://localhost:8000/v1/models
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### å•æ–‡ä»¶ç¿»è¯‘

```bash
sactor translate \
  /path/to/file.c \
  /path/to/test_task.json \
  -r /path/to/result \
  --type bin
```

### æ‰¹é‡ç¿»è¯‘

```bash
python3 batch_translate_generic.py \
    --c-files /path/to/c_files \
    --json-files /path/to/json_files \
    --output /path/to/output \
    --workers 4
```

## ğŸ”§ å¸¸è§é—®é¢˜

**Q: vLLM æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Ÿ**  
A: ç¡®ä¿å·²å®‰è£… vLLM å’Œæ‰€éœ€çš„ä¾èµ–ï¼ˆCUDAã€PyTorch ç­‰ï¼‰

**Q: è¿æ¥è¢«æ‹’ç»ï¼Ÿ**  
A: æ£€æŸ¥ç«¯å£æ˜¯å¦æ­£ç¡®ï¼Œç¡®è®¤ vLLM æœåŠ¡æ­£åœ¨è¿è¡Œ

**Q: GPU å†…å­˜ä¸è¶³ï¼Ÿ**  
A: ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–é™ä½ `gpu-memory-utilization` å‚æ•°

**Q: æ¨¡å‹ä¸‹è½½æ…¢ï¼Ÿ**  
A: é¦–æ¬¡ä½¿ç”¨éœ€è¦ä» HuggingFace ä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥è®¾ç½®é•œåƒæˆ–ä½¿ç”¨ä»£ç†

## ğŸ“š æ›´å¤šä¿¡æ¯

æŸ¥çœ‹ [VLLM_USAGE.md](./VLLM_USAGE.md) è·å–è¯¦ç»†æ–‡æ¡£ã€‚

