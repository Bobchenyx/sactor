#!/usr/bin/env python3
"""
ä»CodeNetæ•°æ®é›†ä¸­éšæœºæŠ½å–æ–‡ä»¶ï¼Œä½¿ç”¨SACToRæ‰¹é‡ç”Ÿæˆæµ‹è¯•
ç»Ÿè®¡æ—¶é—´ã€è°ƒç”¨æ¬¡æ•°å’ŒAPIæˆæœ¬
"""

import os
import subprocess
import json
import time
import random
import glob
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

class RandomTestGenerator:
    def __init__(self):
        self.codenet_base = "/home/changdi/CodeNet/Project_CodeNet/data"
        self.output_dir = "/home/changdi/sactor/random_test_results"
        self.config_file = "/home/changdi/sactor/sactor.toml"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "test_samples"), exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        
        # è¯»å–é…ç½®ä¿¡æ¯
        self._load_config()
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ¤– å½“å‰æ¨¡å‹: {self.current_llm} - {self.current_model}")
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶ä¿¡æ¯"""
        try:
            import toml
            with open(self.config_file, 'r') as f:
                config = toml.load(f)
            
            self.current_llm = config['general']['llm']
            
            if self.current_llm == 'Qwen' and 'Qwen' in config:
                self.current_model = config['Qwen']['model']
            elif self.current_llm == 'OpenAI' and 'OpenAI' in config:
                self.current_model = config['OpenAI']['model']
            else:
                self.current_model = "Unknown"
                
        except Exception as e:
            self.current_llm = "Unknown"
            self.current_model = "Unknown"
            print(f"âš ï¸ è¯»å–é…ç½®å¤±è´¥: {e}")
    
    def find_c_files(self) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰Cæ–‡ä»¶"""
        pattern = os.path.join(self.codenet_base, "*", "C", "*.c")
        c_files = glob.glob(pattern)
        print(f"ğŸ” æ‰¾åˆ° {len(c_files)} ä¸ªCæ–‡ä»¶")
        return c_files
    
    def analyze_c_file(self, c_file_path: str) -> Dict:
        """åˆ†æCæ–‡ä»¶ï¼Œç¡®å®šç¨‹åºç±»å‹"""
        try:
            with open(c_file_path, 'r') as f:
                content = f.read()
            
            analysis = {
                'file_path': c_file_path,
                'file_size': len(content),
                'has_argv': 'argv[' in content and 'argc' in content,
                'has_scanf': 'scanf(' in content,
                'has_main': 'main(' in content,
                'program_type': 'unknown'
            }
            
            if analysis['has_argv']:
                analysis['program_type'] = 'argv'
            elif analysis['has_scanf']:
                analysis['program_type'] = 'scanf'
            elif analysis['has_main']:
                analysis['program_type'] = 'main_only'
            
            return analysis
            
        except Exception as e:
            return {
                'file_path': c_file_path,
                'error': str(e),
                'program_type': 'error'
            }
    
    def generate_test_for_file(self, c_file_path: str, num_tests: int = 10) -> Dict:
        """ä¸ºå•ä¸ªCæ–‡ä»¶ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        c_filename = os.path.basename(c_file_path)
        c_name = c_filename.replace('.c', '')
        
        # åˆ†ææ–‡ä»¶
        analysis = self.analyze_c_file(c_file_path)
        
        if analysis.get('error'):
            return {
                "success": False,
                "error": f"File analysis failed: {analysis['error']}",
                "file_info": analysis
            }
        
        # ç¡®å®šç¨‹åºç±»å‹å’Œè¾“å…¥æ–¹å¼
        if analysis['program_type'] == 'argv':
            feed_mode = "--feed-as-args"
        elif analysis['program_type'] == 'scanf':
            feed_mode = "--feed-as-stdin"
        else:
            # å°è¯•argvæ¨¡å¼ä½œä¸ºé»˜è®¤
            feed_mode = "--feed-as-args"
        
        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_test_samples = os.path.join(self.output_dir, "test_samples", f"{c_name}_test_samples.json")
        output_test_task = os.path.join(self.output_dir, "test_samples", f"{c_name}_test_task.json")
        
        # æ„å»ºDockerå‘½ä»¤
        cmd = [
            "docker", "run", "--rm",
            "-v", f"{os.path.dirname(c_file_path)}:/data",
            "-v", f"{self.config_file}:/app/sactor_1.toml",
            "-v", f"{self.output_dir}:/app/output",
            "sactor", "generate-tests",
            f"/data/{c_filename}",
            str(num_tests),
            "--type", "bin",
            feed_mode,
            "--out-test-sample-path", f"/app/output/test_samples/{c_name}_test_samples.json",
            "--out-test-task-path", f"/app/output/test_samples/{c_name}_test_task.json"
        ]
        
        print(f"ğŸš€ ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {c_filename}")
        print(f"   ç¨‹åºç±»å‹: {analysis['program_type']}")
        print(f"   æ–‡ä»¶å¤§å°: {analysis['file_size']} å­—ç¬¦")
        print(f"   è¾“å…¥æ–¹å¼: {feed_mode}")
        
        start_time = time.time()
        api_calls = 0
        api_cost = 0.0
        
        # é¢„å…ˆä¼°ç®—APIè°ƒç”¨æ¬¡æ•°ï¼ˆåœ¨tryä¹‹å‰ï¼Œé¿å…è¶…æ—¶æ—¶æœªå®šä¹‰ï¼‰
        estimated_calls = num_tests // 5 + 1  # æ¯5ä¸ªæµ‹è¯•ç”¨ä¾‹å¤§çº¦1æ¬¡APIè°ƒç”¨
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            processing_time = time.time() - start_time
            
            # ä»è¾“å‡ºä¸­æå–ä¿¡æ¯
            output_text = result.stdout + result.stderr
            
            # ä½¿ç”¨APIæˆæœ¬ä¼°ç®—
            # å‡è®¾æ¯æ¬¡è°ƒç”¨å¹³å‡ä½¿ç”¨1000è¾“å…¥tokens + 500è¾“å‡ºtokens
            input_tokens = estimated_calls * 1000
            output_tokens = estimated_calls * 500
            
            # æ ¹æ®æ¨¡å‹è®¡ç®—æˆæœ¬
            if self.current_llm == 'Qwen':
                # Qwen3-coder-pluså®šä»· (2025å¹´)
                # Input: Â¥0.008/1K tokens, Output: Â¥0.02/1K tokens
                api_cost = (input_tokens / 1000) * 0.008 + (output_tokens / 1000) * 0.02
            elif self.current_llm == 'OpenAI':
                # GPT-5å®šä»· (2025å¹´)
                # Input: $0.0025/1K tokens, Output: $0.01/1K tokens
                api_cost = (input_tokens / 1000) * 0.0025 + (output_tokens / 1000) * 0.01
            else:
                # é»˜è®¤ä½¿ç”¨GPT-5å®šä»·
                api_cost = (input_tokens / 1000) * 0.0025 + (output_tokens / 1000) * 0.01
            
            if result.returncode == 0:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
                if os.path.exists(output_test_samples):
                    with open(output_test_samples, 'r') as f:
                        test_samples = json.load(f)
                    
                    return {
                        "success": True,
                        "c_file": c_filename,
                        "file_info": analysis,
                        "test_count": len(test_samples),
                        "processing_time": processing_time,
                        "api_calls": estimated_calls,
                        "api_cost": api_cost,
                        "output_files": {
                            "test_samples": output_test_samples,
                            "test_task": output_test_task
                        },
                        "docker_output": output_text[:500]  # ä¿å­˜éƒ¨åˆ†è¾“å‡ºç”¨äºè°ƒè¯•
                    }
                else:
                    return {
                        "success": False,
                        "error": "Output file not generated",
                        "processing_time": processing_time,
                        "api_calls": estimated_calls,
                        "api_cost": api_cost,
                        "docker_output": output_text[:500]
                    }
            else:
                return {
                    "success": False,
                    "error": f"Docker command failed: {result.stderr[:200]}",
                    "processing_time": processing_time,
                    "api_calls": estimated_calls,
                    "api_cost": api_cost,
                    "docker_output": output_text[:500]
                }
                
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Timeout (120 seconds)",
                "processing_time": 120,
                "api_calls": estimated_calls,
                "api_cost": api_cost
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Exception: {str(e)}",
                "processing_time": time.time() - start_time,
                "api_calls": estimated_calls,
                "api_cost": api_cost
            }
    
    def random_sample_and_generate(self, sample_size: int = 20, tests_per_file: int = 10) -> Dict:
        """éšæœºæŠ½æ ·å¹¶ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        print(f"ğŸ² éšæœºæŠ½æ · {sample_size} ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•ç”Ÿæˆ")
        
        # æŸ¥æ‰¾æ‰€æœ‰Cæ–‡ä»¶
        all_c_files = self.find_c_files()
        
        if len(all_c_files) == 0:
            return {"error": "No C files found"}
        
        # éšæœºæŠ½æ ·
        if len(all_c_files) > sample_size:
            sampled_files = random.sample(all_c_files, sample_size)
        else:
            sampled_files = all_c_files
            sample_size = len(all_c_files)
        
        print(f"ğŸ“Š æŠ½æ ·ç»“æœ: {sample_size} ä¸ªæ–‡ä»¶")
        
        results = {
            'sample_size': sample_size,
            'tests_per_file': tests_per_file,
            'total_files': len(sampled_files),
            'llm_provider': self.current_llm,
            'model_name': self.current_model,
            'success': 0,
            'failed': 0,
            'total_tests_generated': 0,
            'total_processing_time': 0,
            'total_api_calls': 0,
            'total_api_cost': 0,
            'program_types': {},
            'details': [],
            'start_time': time.time()
        }
        
        for i, c_file_path in enumerate(sampled_files):
            print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ {i+1}/{len(sampled_files)}: {os.path.basename(c_file_path)}")
            
            result = self.generate_test_for_file(c_file_path, tests_per_file)
            
            if result['success']:
                results['success'] += 1
                results['total_tests_generated'] += result['test_count']
                print(f"âœ… æˆåŠŸç”Ÿæˆ {result['test_count']} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            else:
                results['failed'] += 1
                print(f"âŒ å¤±è´¥: {result['error']}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            results['total_processing_time'] += result['processing_time']
            results['total_api_calls'] += result['api_calls']
            results['total_api_cost'] += result['api_cost']
            
            # ç¨‹åºç±»å‹ç»Ÿè®¡
            program_type = result.get('file_info', {}).get('program_type', 'unknown')
            if program_type not in results['program_types']:
                results['program_types'][program_type] = 0
            results['program_types'][program_type] += 1
            
            results['details'].append(result)
            
            # æ¯å¤„ç†5ä¸ªæ–‡ä»¶ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 5 == 0:
                self._save_progress(results, i + 1)
        
        results['end_time'] = time.time()
        results['duration'] = results['end_time'] - results['start_time']
        
        # è®¡ç®—å¹³å‡å€¼
        if results['total_files'] > 0:
            results['avg_processing_time'] = results['total_processing_time'] / results['total_files']
            results['avg_api_calls'] = results['total_api_calls'] / results['total_files']
            results['avg_api_cost'] = results['total_api_cost'] / results['total_files']
            results['avg_tests_per_file'] = results['total_tests_generated'] / results['success'] if results['success'] > 0 else 0
        
        return results
    
    def _save_progress(self, results: Dict, processed_count: int):
        """ä¿å­˜è¿›åº¦"""
        progress_file = os.path.join(self.output_dir, "logs", f"progress_{processed_count}.json")
        with open(progress_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {processed_count}/{results['total_files']} æ–‡ä»¶")
    
    def log_translation_result(self, result: Dict):
        """è®°å½•ç¿»è¯‘ç»“æœåˆ°æ—¥å¿—æ–‡ä»¶"""
        try:
            log_dir = os.path.join(self.output_dir, "logs")
            os.makedirs(log_dir, exist_ok=True)
            
            # åˆ›å»ºæ—¥å¿—æ–‡ä»¶åï¼ˆåŸºäºæ—¥æœŸï¼‰
            today = datetime.now().strftime("%Y-%m-%d")
            log_file = os.path.join(log_dir, f"random_test_generation_{today}.json")
            csv_log_file = os.path.join(log_dir, f"random_test_generation_{today}.csv")
            
            # å‡†å¤‡æ—¥å¿—æ¡ç›®
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "c_file": result.get('c_file', 'unknown'),
                "success": result.get('success', False),
                "processing_time": result.get('processing_time', 0),
                "api_calls": result.get('api_calls', 0),
                "api_cost": result.get('api_cost', 0),
                "test_count": result.get('test_count', 0),
                "program_type": result.get('file_info', {}).get('program_type', 'unknown'),
                "file_size": result.get('file_info', {}).get('file_size', 0),
                "error": result.get('error', None)
            }
            
            # è¯»å–ç°æœ‰æ—¥å¿—æˆ–åˆ›å»ºæ–°æ—¥å¿—
            if os.path.exists(log_file):
                with open(log_file, 'r') as f:
                    log_data = json.load(f)
            else:
                log_data = {
                    "session_info": {
                        "start_time": datetime.now().isoformat(),
                        "output_dir": self.output_dir
                    },
                    "results": []
                }
            
            # æ·»åŠ æ–°æ¡ç›®
            log_data["results"].append(log_entry)
            log_data["session_info"]["last_update"] = datetime.now().isoformat()
            
            # ä¿å­˜JSONæ—¥å¿—
            with open(log_file, 'w') as f:
                json.dump(log_data, f, indent=2)
            
            # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºæ ‡é¢˜è¡Œ
            if not os.path.exists(csv_log_file):
                with open(csv_log_file, 'w') as f:
                    f.write("timestamp,c_file,success,processing_time,api_calls,api_cost,test_count,program_type,file_size,error\n")
            
            # è¿½åŠ CSVæ¡ç›®
            with open(csv_log_file, 'a') as f:
                error_str = str(log_entry["error"]).replace(',', ';').replace('\n', ' ') if log_entry["error"] else ""
                f.write(f"{log_entry['timestamp']},{log_entry['c_file']},{log_entry['success']},{log_entry['processing_time']:.2f},{log_entry['api_calls']},{log_entry['api_cost']:.4f},{log_entry['test_count']},{log_entry['program_type']},{log_entry['file_size']},{error_str}\n")
            
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—ä¿å­˜å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ CodeNet éšæœºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨")
    print("=" * 60)
    
    generator = RandomTestGenerator()
    
    try:
        # éšæœºæŠ½æ ·å¹¶ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        results = generator.random_sample_and_generate(sample_size=500, tests_per_file=8)
        
        if "error" in results:
            print(f"âŒ é”™è¯¯: {results['error']}")
            return
        
        # è¾“å‡ºç»“æœç»Ÿè®¡
        print(f"\nğŸ“Š éšæœºæµ‹è¯•ç”Ÿæˆç»“æœ:")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {results['llm_provider']} - {results['model_name']}")
        print(f"æŠ½æ ·æ–‡ä»¶æ•°: {results['sample_size']}")
        print(f"æ¯æ–‡ä»¶æµ‹è¯•æ•°: {results['tests_per_file']}")
        print(f"ç”ŸæˆæˆåŠŸ: {results['success']}")
        print(f"ç”Ÿæˆå¤±è´¥: {results['failed']}")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹æ•°: {results['total_tests_generated']}")
        print(f"æ€»å¤„ç†æ—¶é—´: {results['duration']:.2f} ç§’")
        print(f"æ€»APIè°ƒç”¨æ¬¡æ•°: {results['total_api_calls']}")
        print(f"æ€»APIæˆæœ¬: Â¥{results['total_api_cost']:.4f}")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {results['avg_processing_time']:.2f} ç§’/æ–‡ä»¶")
        print(f"å¹³å‡APIè°ƒç”¨: {results['avg_api_calls']:.1f} æ¬¡/æ–‡ä»¶")
        print(f"å¹³å‡APIæˆæœ¬: Â¥{results['avg_api_cost']:.4f}/æ–‡ä»¶")
        print(f"å¹³å‡æµ‹è¯•ç”¨ä¾‹æ•°: {results['avg_tests_per_file']:.1f} ä¸ª/æ–‡ä»¶")
        print(f"æˆåŠŸç‡: {results['success']/results['total_files']*100:.1f}%")
        
        print(f"\nğŸ“ˆ ç¨‹åºç±»å‹ç»Ÿè®¡:")
        for program_type, count in results['program_types'].items():
            percentage = count / results['total_files'] * 100
            print(f"  {program_type}: {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(generator.output_dir, "logs", "random_generation_results.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"ğŸ“ ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ä¿å­˜åœ¨: {generator.output_dir}/test_samples/")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨: {generator.output_dir}/logs/")
        
        return results
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
